{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"02-logistic-word-embeddings_TODO.ipynb","provenance":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"5qDEci8H5EOV"},"source":["# Lab 02: Stance Detection with Logistic Regression and Word Embeddings using scikit-learn\n","\n","In this lab session we will implement a very simple Logistic Regression Classifier for Stance Detection using https://scikit-learn.org\n","\n","Stance detection consists of classifying a given document as expressing an AGAINST, FAVOR or NEUTRAL attitude/stance with respect to a given topic. In this particular lab, we use the Task A data from the Semeval 2016 Twitter dataset for Stance detection: https://alt.qcri.org/semeval2016/task6/ \n","\n","Scikit-learn allows you to quickly experiment with a large number of machine learning algorithms in low resource environments (in comparison to neural network approaches). Scikit-learn also provides a large number of functionalities to process data and evaluate and visualize the obtained results.\n","\n","Unlike other toolkits we will see during the course, scikit-learn is a library with an easy to use API ideal for quick experimentation with a large variety of models and algorithms. Usually, it is a good starting point for classification tasks."]},{"cell_type":"markdown","metadata":{"id":"3HHfi_b8_Jud"},"source":["\n","## ADD DOCUMENTATION\n","\n","+ TODO: Add inline documentation to the notebook cells, this way you can learn what it does.\n"]},{"cell_type":"markdown","metadata":{"id":"LeNHzFZVPH8X"},"source":["## Functions for loading and pre-processing the corpus"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_f08CmiOi9hl","executionInfo":{"status":"ok","timestamp":1626083671262,"user_tz":-120,"elapsed":20341,"user":{"displayName":"Rodrigo Agerri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW2M3VpBFpNNhe0tx607nu7jJN85RXoeaB4nSw=s64","userId":"18226337086247956839"}},"outputId":"07813e98-7b01-4f8e-f4eb-5e72253f19d6"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TvjWbWcwZf1W","executionInfo":{"status":"ok","timestamp":1626083672778,"user_tz":-120,"elapsed":1530,"user":{"displayName":"Rodrigo Agerri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW2M3VpBFpNNhe0tx607nu7jJN85RXoeaB4nSw=s64","userId":"18226337086247956839"}},"outputId":"16dcaf31-2d7c-4c3c-c158-e6854ad2b492"},"source":["import pandas as pd\n","import nltk\n","import numpy as np\n","\n","nltk.download('stopwords')\n","\n","# load data\n","def load_data(fnames):\n","    data = []\n","    for fname in fnames:\n","        data.append(pd.read_csv(fname, sep='\\t', encoding='utf-8'))\n","    data = pd.concat(data)\n","    targets = set(data['Target'])\n","    return data, list(targets)\n","\n","def tokenized_tweets(df):\n","    \"\"\"Function to tokenize the tweets using the tokenizer from NLTK\"\"\"\n","    tknzr = nltk.TweetTokenizer()\n","    df['Tokenized_tweet'] = df['Tweet'].apply(tknzr.tokenize)\n","    return df"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","source":["# ASSIGNMENT 1\n","\n","+ TODO: describe how the function loading the word embeddings works. What is the input, the output and how it is obtained. HINT: Inspect the embedding glove twitter lexicon used in Assignment 2."],"metadata":{"id":"-vEOTT3EyOwb"}},{"cell_type":"code","metadata":{"id":"RN1d_QzWZf1g"},"source":["from nltk.corpus import stopwords\n","import string\n","from sklearn import preprocessing\n","from sklearn.feature_extraction import DictVectorizer\n","\n","def preprocess(data, tokenize=True, remove_stopwords=True, remove_none=True):\n","    '''TODO describe what this function does'''\n","    if tokenize:\n","        data = tokenized_tweets(data)\n","        data['Clean_tweet'] = data['Tokenized_tweet']\n","    if remove_stopwords:\n","        stop = stopwords.words('english')\n","        data['Clean_tweet'] = data['Clean_tweet'].apply(lambda sentence: [word for word in sentence if word not in stop])\n","        data['Clean_tweet'] = data['Clean_tweet'].apply(lambda sentence: [word for word in sentence if not all([c in string.punctuation for c in word])])\n","    if remove_none:\n","        data = data[data['Stance'] != 'NONE']\n","    return data[['Target','Clean_tweet', 'Stance']]   \n","    \n","def read_glove(path, dim):\n","    '''\n","    read the glove vectors from path with dimension dim\n","    '''\n","    df = pd.read_csv(path, sep=\" \", quoting=3, header=None, index_col=0)\n","    glove = {key: val.values for key, val in df.T.items()}\n","    return glove\n","\n","# TODO: provide a description of this function. HINT: it helps to look at the glove twitter word embeddings\n","# used below.    \n","def gloveVectorize(glove, text):\n","    '''\n","    TODO: function doc\n","    '''\n","    dim = len(glove[\"the\"])\n","    X = np.zeros( (len(text), dim) )\n","    for text_id, t in enumerate(text):\n","        tmp = np.zeros((1, dim))        \n","        # remove oov words\n","        words = [w for w in t if w in glove.keys()]\n","        for word in words:\n","            tmp[:] += glove[word]\n","\n","        if len(words) == 0:\n","            X[text_id, :] = np.zeros((1, dim)) \n","        else:\n","            X[text_id, :] = tmp/len(words)\n","    return X\n","\n","def encode_labels(labels):\n","    enc = preprocessing.LabelEncoder()\n","    encoded = enc.fit_transform(labels)\n","    decoded = enc.inverse_transform(encoded)\n","    return encoded, decoded\n","\n","def data_as_numpy(data):\n","    return np.asarray(data['Clean_tweet']), np.asarray(data['Stance'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":831},"id":"T8HgyqeUZf1i","executionInfo":{"status":"ok","timestamp":1626083675981,"user_tz":-120,"elapsed":3218,"user":{"displayName":"Rodrigo Agerri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW2M3VpBFpNNhe0tx607nu7jJN85RXoeaB4nSw=s64","userId":"18226337086247956839"}},"outputId":"9e7c5b64-2671-4601-c61f-e3e067a2c9a4"},"source":["# TASK A in-target supervised: \n","# load train / test\n","trial_file = \"/content/drive/My Drive/Colab Notebooks/2022-ILTAPP/datasets/stance-semeval2016/semeval2016-task6-trialdata.utf-8.txt\"\n","train_file = \"/content/drive/My Drive/Colab Notebooks/2022-ILTAPP/datasets/stance-semeval2016/semeval2016-task6-trainingdata.utf-8.txt\"\n","test_file = \"/content/drive/My Drive/Colab Notebooks/2022-ILTAPP/datasets/stance-semeval2016/SemEval2016-Task6-subtaskA-testdata-gold.txt\"\n","\n","training_data, targets = load_data([train_file, trial_file])\n","test_data, _ = load_data([test_file])\n","\n","# show original training data\n","display(training_data)\n","\n","#training_data[training_data['Target'] == targets[0]].head()\n","\n","# preprocess\n","training_preproc_data = preprocess(training_data, remove_none=False)\n","test_preproc_data = preprocess(test_data, remove_none=False )\n","\n","# show clean training data\n","display(training_preproc_data)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Target</th>\n","      <th>Tweet</th>\n","      <th>Stance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>101</td>\n","      <td>Atheism</td>\n","      <td>dear lord thank u for all of ur blessings forg...</td>\n","      <td>AGAINST</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>102</td>\n","      <td>Atheism</td>\n","      <td>Blessed are the peacemakers, for they shall be...</td>\n","      <td>AGAINST</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>103</td>\n","      <td>Atheism</td>\n","      <td>I am not conformed to this world. I am transfo...</td>\n","      <td>AGAINST</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>104</td>\n","      <td>Atheism</td>\n","      <td>Salah should be prayed with #focus and #unders...</td>\n","      <td>AGAINST</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>105</td>\n","      <td>Atheism</td>\n","      <td>And stay in your houses and do not display you...</td>\n","      <td>AGAINST</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>96</td>\n","      <td>Legalization of Abortion</td>\n","      <td>@Corey_Frizzell @PEILiberalParty and most Isla...</td>\n","      <td>NONE</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>97</td>\n","      <td>Legalization of Abortion</td>\n","      <td>@Docjp Pressure? It's their job and they are f...</td>\n","      <td>NONE</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>98</td>\n","      <td>Legalization of Abortion</td>\n","      <td>I love how #liberals only accuse #conservative...</td>\n","      <td>AGAINST</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>99</td>\n","      <td>Legalization of Abortion</td>\n","      <td>Help your friend figure out how they're going ...</td>\n","      <td>NONE</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>100</td>\n","      <td>Legalization of Abortion</td>\n","      <td>@JimDDaniels1 Plenty of reasons for Christians...</td>\n","      <td>AGAINST</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2914 rows × 4 columns</p>\n","</div>"],"text/plain":["     ID  ...   Stance\n","0   101  ...  AGAINST\n","1   102  ...  AGAINST\n","2   103  ...  AGAINST\n","3   104  ...  AGAINST\n","4   105  ...  AGAINST\n","..  ...  ...      ...\n","95   96  ...     NONE\n","96   97  ...     NONE\n","97   98  ...  AGAINST\n","98   99  ...     NONE\n","99  100  ...  AGAINST\n","\n","[2914 rows x 4 columns]"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Target</th>\n","      <th>Clean_tweet</th>\n","      <th>Stance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Atheism</td>\n","      <td>[dear, lord, thank, u, ur, blessings, forgive,...</td>\n","      <td>AGAINST</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Atheism</td>\n","      <td>[Blessed, peacemakers, shall, called, children...</td>\n","      <td>AGAINST</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Atheism</td>\n","      <td>[I, conformed, world, I, transformed, renewing...</td>\n","      <td>AGAINST</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Atheism</td>\n","      <td>[Salah, prayed, #focus, #understanding, #Allah...</td>\n","      <td>AGAINST</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Atheism</td>\n","      <td>[And, stay, houses, display, like, times, igno...</td>\n","      <td>AGAINST</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>Legalization of Abortion</td>\n","      <td>[@Corey_Frizzell, @PEILiberalParty, Islanders,...</td>\n","      <td>NONE</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>Legalization of Abortion</td>\n","      <td>[@Docjp, Pressure, It's, job, failing, miserab...</td>\n","      <td>NONE</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>Legalization of Abortion</td>\n","      <td>[I, love, #liberals, accuse, #conservatives, w...</td>\n","      <td>AGAINST</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>Legalization of Abortion</td>\n","      <td>[Help, friend, figure, they're, going, get, st...</td>\n","      <td>NONE</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>Legalization of Abortion</td>\n","      <td>[@JimDDaniels1, Plenty, reasons, Christians, s...</td>\n","      <td>AGAINST</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2914 rows × 3 columns</p>\n","</div>"],"text/plain":["                      Target  ...   Stance\n","0                    Atheism  ...  AGAINST\n","1                    Atheism  ...  AGAINST\n","2                    Atheism  ...  AGAINST\n","3                    Atheism  ...  AGAINST\n","4                    Atheism  ...  AGAINST\n","..                       ...  ...      ...\n","95  Legalization of Abortion  ...     NONE\n","96  Legalization of Abortion  ...     NONE\n","97  Legalization of Abortion  ...  AGAINST\n","98  Legalization of Abortion  ...     NONE\n","99  Legalization of Abortion  ...  AGAINST\n","\n","[2914 rows x 3 columns]"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"A-IXZCc3-sPK"},"source":[""]},{"cell_type":"markdown","source":["# ASSIGNMENT 2\n","\n","+ TODO try with different word embeddings. For example:\n","  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz (1.2GB)\n"],"metadata":{"id":"ngSIQqWEyiSE"}},{"cell_type":"code","metadata":{"id":"KPMTIjSkZf1j"},"source":["#TODO try with the different embeddings in resources directory and see which one obtains better results\n","\n","# set path for word embeddings and vectorize\n","pretrained_wv_path = '/content/drive/My Drive/Colab Notebooks/2022-ILTAPP/resources/glove.twitter.27B.25d.txt.gz'\n","\n","glove = read_glove(pretrained_wv_path, 300)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HMfHsNO3Zf1k"},"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","\n","def fit_lr(train_x, train_y, c=1.0, weights='equal'):\n","    if weights == 'equal':\n","        logreg = Pipeline([\n","            (\"scaler\", StandardScaler()),\n","            (\"logit\", LogisticRegression(C=c, solver=\"lbfgs\", max_iter=1000))\n","        ])\n","    else:\n","        logreg = Pipeline([\n","        (\"scaler\", StandardScaler()),\n","        (\"logit\", LogisticRegression(C=c, solver=\"lbfgs\", class_weight=weights, max_iter=1000))\n","        ])\n","    logreg.fit(train_x, train_y)\n","    return logreg"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ASSIGNMENT 3\n","\n","+ TODO: evaluate accuracy, F1 macro, F1 micro using sklearn functions. HINT: Check previous labs."],"metadata":{"id":"0oNE_vPNqRa9"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":597},"id":"Mmm98JcHZf1l","executionInfo":{"status":"ok","timestamp":1626083706595,"user_tz":-120,"elapsed":39,"user":{"displayName":"Rodrigo Agerri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW2M3VpBFpNNhe0tx607nu7jJN85RXoeaB4nSw=s64","userId":"18226337086247956839"}},"outputId":"9fe07877-9a3e-43d4-fb45-14ff6f98f72f"},"source":["best_c = 1.2\n","class_weight = 'balanced'\n","\n","# prediction map\n","label_map = {0: 'AGAINST',\n","             1: 'FAVOR',\n","             2: 'NONE'}\n","\n","predictions = pd.DataFrame()\n","\n","training_texts, training_labels = data_as_numpy(training_preproc_data)\n","train_x = gloveVectorize(glove, training_texts)\n","train_y, labels = encode_labels(training_labels)\n","\n","test_texts, test_labels = data_as_numpy(test_preproc_data)\n","test_x = gloveVectorize(glove, test_texts)\n","test_y, labels = encode_labels(test_labels)\n","\n","# fit model\n","logreg = fit_lr(train_x, train_y, c=best_c, weights=class_weight)\n","\n","# predict\n","stance = logreg.predict(test_x)\n","stance_probs = logreg.predict_proba(test_x)\n","\n","# TODO evaluate accuracy, F1 macro, F1 micro using sklearn functions\n","\n","predictions = test_data[['ID', 'Target', 'Tweet']]\n","predictions_probs = test_data[['ID', 'Target', 'Tweet']]\n","predictions['Stance'] = [label_map[s] for s in stance]\n","predictions_probs['Stance'] = [s for s in stance_probs]\n","\n","predictions = predictions.sort_values(by='ID')\n","predictions_probs = predictions_probs.sort_values(by='ID')\n","\n","display(predictions)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Target</th>\n","      <th>Tweet</th>\n","      <th>Stance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10001</td>\n","      <td>Atheism</td>\n","      <td>He who exalts himself shall      be humbled; a...</td>\n","      <td>AGAINST</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>10002</td>\n","      <td>Atheism</td>\n","      <td>RT @prayerbullets: I remove Nehushtan -previou...</td>\n","      <td>NONE</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10003</td>\n","      <td>Atheism</td>\n","      <td>@Brainman365 @heidtjj @BenjaminLives I have so...</td>\n","      <td>AGAINST</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>10004</td>\n","      <td>Atheism</td>\n","      <td>#God is utterly powerless without Human interv...</td>\n","      <td>FAVOR</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10005</td>\n","      <td>Atheism</td>\n","      <td>@David_Cameron   Miracles of #Multiculturalism...</td>\n","      <td>NONE</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1244</th>\n","      <td>11245</td>\n","      <td>Legalization of Abortion</td>\n","      <td>@MetalheadMonty @tom_six I followed him before...</td>\n","      <td>AGAINST</td>\n","    </tr>\n","    <tr>\n","      <th>1245</th>\n","      <td>11246</td>\n","      <td>Legalization of Abortion</td>\n","      <td>For he who avenges blood remembers, he does no...</td>\n","      <td>AGAINST</td>\n","    </tr>\n","    <tr>\n","      <th>1246</th>\n","      <td>11247</td>\n","      <td>Legalization of Abortion</td>\n","      <td>Life is sacred on all levels. Abortion does no...</td>\n","      <td>AGAINST</td>\n","    </tr>\n","    <tr>\n","      <th>1247</th>\n","      <td>11248</td>\n","      <td>Legalization of Abortion</td>\n","      <td>@ravensymone U refer to \"WE\" which =\"YOU\" &amp; a ...</td>\n","      <td>FAVOR</td>\n","    </tr>\n","    <tr>\n","      <th>1248</th>\n","      <td>11249</td>\n","      <td>Legalization of Abortion</td>\n","      <td>Al Robertson's mom #DuckDynasty  chose life as...</td>\n","      <td>NONE</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1249 rows × 4 columns</p>\n","</div>"],"text/plain":["         ID  ...   Stance\n","0     10001  ...  AGAINST\n","1     10002  ...     NONE\n","2     10003  ...  AGAINST\n","3     10004  ...    FAVOR\n","4     10005  ...     NONE\n","...     ...  ...      ...\n","1244  11245  ...  AGAINST\n","1245  11246  ...  AGAINST\n","1246  11247  ...  AGAINST\n","1247  11248  ...    FAVOR\n","1248  11249  ...     NONE\n","\n","[1249 rows x 4 columns]"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","source":["# (BONUS) ASSIGNMENT 4\n","\n","Check the Feature-based lab and perform the following steps:\n","\n","+ TODO: Change code from above to use SVM instead of logistic regression.\n","+ TODO modify code to run over every target in the dataframes for training and test.\n","\n","Write a table to:\n","+ TODO: Compare results with logistic regression and SVM using word embeddings on the test set.\n","+ TODO: Compare results with feature-based SVM on the test set."],"metadata":{"id":"q_7Y404Oz42q"}}]}