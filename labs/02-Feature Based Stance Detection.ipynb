{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"02-Feature Based Stance Detection.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"fhxI1uz7gipS"},"source":["# Local Feature Based Stance Detection\n","\n","In this lab session we will implement a very simple linear classifier for Stance Detection using https://scikit-learn.org\n","\n","Stance detection consists of classifying a given document as expressing an AGAINST, FAVOR or NEUTRAL attitude/stance with respect to a given topic. In this particular lab, we use the Task A data from the Semeval 2016 Twitter dataset for Stance detection: https://alt.qcri.org/semeval2016/task6/ \n","\n","Scikit-learn allows you to quickly experiment with a large number of machine learning algorithms in low resource environments (in comparison to neural network approaches). Scikit-learn also provides a large number of functionalities to process data and evaluate and visualize the obtained results.\n","\n","Unlike other toolkits we will see during the course, scikit-learn is a library with an easy to use API ideal for quick experimentation with a large variety of models and algorithms. Usually, it is a good starting point for classification tasks.\n","\n","REMEMBER to check the tutorial:\n","https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html# "]},{"cell_type":"markdown","metadata":{"id":"nrXPDeiBgipd"},"source":["## Load data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eRedGVJFHSdv","executionInfo":{"status":"ok","timestamp":1644674401742,"user_tz":-60,"elapsed":17057,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"2c48e5a3-612b-4eca-ac4b-1d4edef2aedb"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/LAP/Subjects/AP1/labs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vEGE4HAt-59I","executionInfo":{"status":"ok","timestamp":1644674402137,"user_tz":-60,"elapsed":399,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"5c2756ef-7827-4fac-acb1-2ef26fcd89e4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/LAP/Subjects/AP1/labs\n"]}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"AOEW0KAHgipe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644674403399,"user_tz":-60,"elapsed":1264,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"c0982f4c-9f55-4d6e-ad78-b38874f38fc2"},"source":["import pandas as pd\n","import nltk\n","import numpy as np\n","from nltk.corpus import stopwords\n","import string\n","from sklearn import preprocessing\n","\n","# download English stopwords\n","nltk.download('stopwords')\n","# download nltk pos tagger for English\n","nltk.download('averaged_perceptron_tagger')\n","\n","# load data\n","def load_data(fnames):\n","    data = []\n","    for fname in fnames:\n","        data.append(pd.read_csv(fname, sep='\\t', encoding='utf-8'))\n","    data = pd.concat(data)\n","    targets = set(data['Target'])\n","    return data, list(targets)\n","\n","def tokenized_tweets(df):\n","    tknzr = nltk.TweetTokenizer()\n","    df['Tokenized_tweet'] = df['Tweet'].apply(tknzr.tokenize)\n","    return df\n","\n","def read_glove(path):\n","    '''\n","    read the glove vectors from path with dimension dim\n","    '''\n","    df = pd.read_csv(path, sep=\" \", quoting=3, header=None, index_col=0)\n","    glove = {key: val.values for key, val in df.T.items()}\n","    return glove\n","\n","def preprocess(data, tokenize=True, remove_stopwords=True, remove_none=True):\n","    if tokenize:\n","        data = tokenized_tweets(data)\n","        data['Clean_tweet'] = data['Tokenized_tweet']\n","    if remove_stopwords:\n","        stop = stopwords.words('english')\n","        data['Clean_tweet'] = data['Clean_tweet'].apply(lambda sentence: [word for word in sentence if word not in stop])\n","        data['Clean_tweet'] = data['Clean_tweet'].apply(lambda sentence: [word for word in sentence if not all([c in string.punctuation for c in word])])\n","    if remove_none:\n","        data = data[data['Stance'] != 'NONE']\n","    return data[['Target','Clean_tweet', 'Stance']]   \n","    \n","def gloveVectorize(glove, text):\n","    '''\n","    Find the pretrained glove vectors of the words in the input text.\n","    The final vector is the average of the vectors\n","    '''\n","    dim = len(glove[\"the\"])\n","    X = np.zeros( (len(text), dim) )\n","    for text_id, t in enumerate(text):\n","        tmp = np.zeros((1, dim))        \n","        # remove oov words\n","        words = [w for w in t if w in glove.keys()]\n","        for word in words:\n","            tmp[:] += glove[word]\n","\n","        if len(words) == 0:\n","            X[text_id, :] = np.zeros((1, dim)) \n","        else:\n","            X[text_id, :] = tmp/len(words)\n","    return X\n","\n","def encode_labels(labels):\n","    enc = preprocessing.LabelEncoder()\n","    encoded = enc.fit_transform(labels)\n","    decoded = enc.inverse_transform(encoded)\n","    return encoded, decoded\n","\n","def data_as_numpy(data):\n","    return np.asarray(data['Tweet']), np.asarray(data['Stance'])\n","\n","def get_table(results):\n","    # print best models results\n","    targts, models, accs = [],[],[]\n","    precs,recs, f1s = [],[], []\n","    par_names, par_values = [], []\n","\n","    for target in results.keys():\n","        for model in results[target]:\n","            targts.append(target)\n","            models.append(model)\n","            accs.append(np.mean(results[target][model]['scores']['test_accuracy']))\n","            precs.append(np.mean(results[target][model]['scores']['test_precision']))\n","            recs.append(np.mean(results[target][model]['scores']['test_recall']))\n","            f1s.append(np.mean(results[target][model]['scores']['test_f1']))\n","            if model == 'rf':\n","                par_names.append('D')\n","                par_values.append(results[target][model]['D'])\n","            else:\n","                par_names.append('C')\n","                par_values.append(results[target][model]['C'])\n","    res_table = pd.DataFrame({'target':targts, 'model':models, \n","                              'accuracy':accs, 'precision':precs, \n","                              'recall':recs, 'fscore':f1s,\n","                              'par_name':par_names, 'par_value':par_values}, columns=['target', 'model', \n","                                                                     'accuracy', 'precision',\n","                                                                     'recall', 'fscore', 'par_name', 'par_value'])\n","    return res_table"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]}]},{"cell_type":"code","metadata":{"id":"UgBYe_3Jgiph","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1644674404957,"user_tz":-60,"elapsed":1561,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"b8004f93-4a5b-4e3b-e8dd-83d2ac3e0a1c"},"source":["# data path. trial data used as training too.\n","folder = \"stance-semeval2016\"\n","trial_file = f\"../datasets/{folder}/semeval2016-task6-trialdata.utf-8.txt\"\n","train_file = f\"../datasets/{folder}/semeval2016-task6-trainingdata.utf-8.txt\"\n","test_file = f\"../datasets/{folder}/SemEval2016-Task6-subtaskA-testdata-gold.txt\"\n","\n","training_data, targets = load_data([trial_file, train_file])\n","training_data = tokenized_tweets(training_data)\n","\n","test_data, targets = load_data([test_file])\n","test_data = tokenized_tweets(test_data)\n","\n","training_data.head()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-b2f6b6cb-d2f6-4821-a008-75be551909a9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Target</th>\n","      <th>Tweet</th>\n","      <th>Stance</th>\n","      <th>Tokenized_tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Hillary Clinton</td>\n","      <td>@tedcruz And, #HandOverTheServer she wiped cle...</td>\n","      <td>AGAINST</td>\n","      <td>[@tedcruz, And, ,, #HandOverTheServer, she, wi...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Hillary Clinton</td>\n","      <td>Hillary is our best choice if we truly want to...</td>\n","      <td>FAVOR</td>\n","      <td>[Hillary, is, our, best, choice, if, we, truly...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Hillary Clinton</td>\n","      <td>@TheView I think our country is ready for a fe...</td>\n","      <td>AGAINST</td>\n","      <td>[@TheView, I, think, our, country, is, ready, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>Hillary Clinton</td>\n","      <td>I just gave an unhealthy amount of my hard-ear...</td>\n","      <td>AGAINST</td>\n","      <td>[I, just, gave, an, unhealthy, amount, of, my,...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Hillary Clinton</td>\n","      <td>@PortiaABoulger Thank you for adding me to you...</td>\n","      <td>NONE</td>\n","      <td>[@PortiaABoulger, Thank, you, for, adding, me,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2f6b6cb-d2f6-4821-a008-75be551909a9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b2f6b6cb-d2f6-4821-a008-75be551909a9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b2f6b6cb-d2f6-4821-a008-75be551909a9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   ID  ...                                    Tokenized_tweet\n","0   1  ...  [@tedcruz, And, ,, #HandOverTheServer, she, wi...\n","1   2  ...  [Hillary, is, our, best, choice, if, we, truly...\n","2   3  ...  [@TheView, I, think, our, country, is, ready, ...\n","3   4  ...  [I, just, gave, an, unhealthy, amount, of, my,...\n","4   5  ...  [@PortiaABoulger, Thank, you, for, adding, me,...\n","\n","[5 rows x 5 columns]"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"TLAneW_1gipj"},"source":["## Feature Extraction\n","\n","From [Mohammad et al. 2016](http://saifmohammad.com/WebDocs/1605.01655v1.pdf):\n","\n","The features used in our text classification system are shown below:\n","\n","- __n-grams__: presence or absence of contiguous sequences of 1, 2 and 3 tokens (word n-grams); presence or absence of contiguous sequences of 2, 3, 4, and 5 characters (character n-grams);\n","- __sentiment (sent.)__: The sentiment lexicon features are derived from three manually created lexicons: NRC Emotion Lexicon [Mohammad and Turney 2010], Hu and Liu Lexicon [Hu and Liu 2004], and MPQA Subjectivity Lexicon [Wilson et al. 2005], and two automatically created, tweet-specific, lexicons: NRC Hashtag Sentiment and\n","NRC Emoticon (a.k.a. Sentiment140) [Kiritchenko et al. 2014a]; \n","  + NRC Emotion Lexicon: https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm\n","  + Hu and Liu Lexicon: https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html\n","  + MPQA Subjectivity Lexicon: https://mpqa.cs.pitt.edu/lexicons/subj_lexicon/\n","  + NRC Hashtag Sentiment and NRC Emoticon (a.k.a. Sentiment140): http://saifmohammad.com/WebPages/lexicons.html\n"," \n","\n","- __target__: presence/absence of the target of interest in the tweet;\n","- __POS__: the number of occurrences of each part-of-speech tag (POS);\n","- __encodings (enc.)__: presence/absence of positive and negative emoticons, hashtags, characters in upper case, elongated words (e.g., sweeettt), and punctuations such as exclamation and question marks.\n","\n","----"]},{"cell_type":"markdown","metadata":{"id":"S-sjtzJcgipl"},"source":["# ASSIGNMENT 1\n","\n","We define some helper functions for the feature extraction.\n","\n","+ TODO: define the function to perform POS tagging using NLTK\n","+ TODO: add code to read the NRC-Hashtag-Sentiment-Lexicon-v1.0"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"5KSiM9JUgipn","executionInfo":{"status":"ok","timestamp":1644674404959,"user_tz":-60,"elapsed":11,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["# utils\n","import re\n","import nltk\n","tknzr = nltk.TweetTokenizer() # keep global for simplicity\n","\n","def tokenize(sentence):\n","    return tknzr.tokenize(sentence)\n","\n","def pos_tag(sentence):\n","    # TODO complete this function to perform pos tagging using NLTK\n","    return nltk.pos_tag(sentence)\n","\n","# helper functions for feature extraction\n","def ngrams(tokens, n):\n","    return list(zip(*[tokens[i:] for i in range(n)]))\n","\n","def load_nrc_emotions(fname):\n","    emotions = {}\n","    f = open(fname, 'r')\n","    _ = f.readline()\n","    for line in f:\n","        word, emotion, affect = line.rstrip().split('\\t')\n","        if affect == '1':\n","            if word not in emotions:\n","                emotions[word] = []\n","            emotions[word].append(emotion)\n","    f.close()\n","    return emotions\n","\n","def load_nrc_hashtags(fname_list):\n","    sentiment = {}\n","    # TODO add code to read the NRC hashtags file\n","    sentiment = {}\n","    for fname in fname_list:\n","        f = open(fname, 'r')\n","        for line in f:\n","            sent, score, _, _ = line.rstrip().split('\\t')\n","            sentiment[sent] = score\n","        f.close()\n","    return sentiment\n","\n","def load_nrc_emoticons(fname_list):\n","    emoticons = {}\n","    for fname in fname_list:\n","        f = open(fname, 'r')\n","        for line in f:\n","            emoti, score, _, _ = line.rstrip().split('\\t')\n","            emoticons[emoti] = score\n","        f.close()\n","    return emoticons\n","\n","def load_hu_liu(neg_fname, pos_fname):\n","    sentiments = {}\n","    f = open(neg_fname, 'r')\n","    for line in f:\n","        if re.search('^;', line.rstrip()):\n","            continue\n","        if re.search('^$', line.rstrip()):\n","            continue\n","        sentiments[line.rstrip()] = 'negative'\n","    f.close()\n","    f = open(pos_fname, 'r')\n","    for line in f:\n","        if re.search('^;', line.rstrip()):\n","            continue\n","        if re.search('^$', line.rstrip()):\n","            continue\n","        sentiments[line.rstrip()] = 'positive'\n","    f.close()\n","    return sentiments\n","\n","def load_mpqa_polarities(fname):\n","    polarities = {}\n","    f = open(fname, 'r')\n","    for line in f:\n","        sp  = line.rstrip().split(' ')\n","        if sp[5] == 'm':\n","            word = sp[2].split('=')[1]\n","            polarity = sp[6].split('=')[1]\n","        else:\n","            word = sp[2].split('=')[1]\n","            polarity = sp[5].split('=')[1]\n","        polarities[word] = polarity\n","    f. close()\n","    return polarities\n","\n","\n","def generate_target_dict(targets):\n","    target_dict = {}\n","    stop = stopwords.words('english')\n","    for target in targets:\n","        # original\n","        target_dict[target] = 1\n","\n","        # lower case\n","        target_dict[target.lower()] = 1\n","\n","        # join Hillary Clinton => HillaryClinton\n","        target_dict[target.replace(' ', '')] = 1\n","        target_dict['#'+target.replace(' ', '')] = 1\n","        target_dict['@'+target.replace(' ', '')] = 1\n","        target_dict[target.lower().replace(' ', '')] = 1\n","        target_dict['#'+target.lower().replace(' ', '')] = 1\n","        target_dict['@'+target.lower().replace(' ', '')] = 1\n","\n","        # process parts of target name:\n","        for part in target.split(' '):\n","            if part not in stop:\n","                target_dict[part] = 1\n","                target_dict['#'+part] = 1\n","                target_dict['@'+part] = 1\n","                target_dict['#'+part.lower()] = 1\n","                target_dict['@'+part.lower()] = 1\n","    return target_dict\n","\n","\n","class Match(object):\n","\n","    def __init__(self):\n","        self.trie = {}\n","\n","    def match(self, words):\n","        i = 0\n","        while (i < len(words)):\n","            j = self.match2(words, i, self.trie)\n","            if j >= 0:\n","                return True\n","            i += 1\n","        return False\n","\n","    def match2(self, words, i, trie):\n","        if words[i] not in trie:\n","            return -1\n","        for length in sorted(trie[words[i]].keys(), reverse=True):\n","            context = ' '.join(words[i+1:i+length+1])\n","            for entry in trie[words[i]][length]:\n","                if context == entry:\n","                    return length\n","        return -1\n","\n","    def matchinit(self, dictionary):\n","        for entry in dictionary.keys():\n","            firstword = re.split(' +', entry)[0]\n","            rwords = re.split(' +', entry)[1:]\n","\n","            length = len(rwords)\n","            if firstword not in self.trie:\n","                self.trie[firstword] = {}\n","            if length not in self.trie[firstword]:\n","                self.trie[firstword][length] = []\n","            self.trie[firstword][length].append(' '.join(rwords))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mRftbPMIgipn"},"source":["----\n","\n","We load emotion and sentiment lexicons used in the feature extraction "]},{"cell_type":"code","metadata":{"collapsed":true,"id":"YfCUkYcdgipo","executionInfo":{"status":"ok","timestamp":1644674409970,"user_tz":-60,"elapsed":5019,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["# Load lexicon and use as global variables.\n","\n","# NRC emotion lexicon\n","nrc_emotion_file = '../datasets/NRC-Emotion-Lexicon-v0.92/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n","nrc_emotions = load_nrc_emotions(nrc_emotion_file)\n","\n","# NRC hashtag sentiment\n","nrc_hashtag_path = '../datasets/NRC-Hashtag-Sentiment-Lexicon-v1.0'\n","nrc_ht_sentiments = load_nrc_hashtags([nrc_hashtag_path+'/HS-unigrams.txt', nrc_hashtag_path+'/HS-bigrams.txt'])\n","\n","# NRC emoticons sentiment\n","nrc_emoticons_path = '../datasets/NRC-Emoticon-Lexicon-v1.0'\n","nrc_em_sentiments = load_nrc_emoticons([nrc_emoticons_path+'/Emoticon-unigrams.txt', nrc_emoticons_path+'/Emoticon-bigrams.txt'])\n","\n","# Hu and Liu sentiment lexicon\n","hu_liu_path = '../datasets/Hu-Liu_sentiment_lexicon'\n","hu_liu_sentiments = load_hu_liu(hu_liu_path+'/negative-words.utf8.txt', hu_liu_path+'/positive-words.utf8.txt')\n","\n","# MPQA polarity lexicon\n","mpqa_file = '../datasets/mpqa_polarities/subjclueslen1-HLTEMNLP05.tff'\n","mpqa_polarities = load_mpqa_polarities(mpqa_file)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rzseby-Cgipp"},"source":["----\n","\n","Define feature functions. Function will apply for each instance in the dataset. Output of the functions is a custom python dictionary with the activated/extracted features. Note that after the extraction we'll need to vectorize whole dataset of feature."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"8no7nzaSgipq","executionInfo":{"status":"ok","timestamp":1644674410418,"user_tz":-60,"elapsed":451,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["# features\n","def word_ngrams(tokens, n):\n","    features = {}\n","    name = str(n)+'wgram:'\n","    for ngram in ngrams(tokens, n):\n","        features[name+'_'.join(ngram)] = 1\n","    return features\n","\n","def char_ngrams(sentence, n):\n","    features = {}\n","    name = str(n)+'cgram:'\n","    for ngram in ngrams(sentence, n):\n","        features[name+'_'.join(ngram)] = 1\n","    return features\n","\n","def pos_nb(pos_tags):\n","    features = {}\n","    name='pos_nb:'\n","    for tag in pos_tags:\n","        feat = name+tag[1]\n","        if feat not in features:\n","            features[feat] = 1\n","        else:\n","            features[feat] += 1\n","    return features\n","\n","def target_occurs(sentence, matcher):\n","    features = {}\n","    name = 'target:'\n","    if  matcher.match(sentence):\n","        features[name+'true'] = 1\n","    else:\n","        features[name+'false'] = 1\n","    return features\n","\n","def nrc_emotions_features(tokens, lexicon):\n","    features = {}\n","    name='nrc_emo:'\n","    for token in tokens:\n","        if token in lexicon:\n","            for emotion in lexicon[token]:\n","                features[name+token+':'+emotion] = 1\n","    return features\n","\n","def nrc_hashtag_features(tokens, lexicon):\n","    features = {}\n","    name = 'nrc_ht:'\n","    for token in tokens:\n","        if token in lexicon:\n","            features[name+token] = lexicon[token]\n","    return features\n","\n","def nrc_emoticons_features(tokens, lexicon):\n","    features = {}\n","    name = 'nrc_emc:'\n","    for token in tokens:\n","        if token in lexicon:\n","            features[name+token] = lexicon[token]\n","    return features\n","\n","def hu_liu_sentiment_features(tokens, lexicon):\n","    features = {}\n","    name = 'hu_liu:'\n","    for token in tokens:\n","        if token in lexicon:\n","            features[name+token+':'+lexicon[token]] = 1\n","    return features\n","\n","def mpqa_polarity_features(tokens, lexicon):\n","    features = {}\n","    name = 'mpqa:'\n","    for token in tokens:\n","        if token in lexicon:\n","            features[name+token+':'+lexicon[token]] = 1\n","    return features\n","\n","\n","def encoding(tokens):\n","    pass"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# ASSIGNMENT 2\n","\n","+ TODO: extract bigram and trigram character ngram features using the \"char_ngram\" function.\n","+ TODO: extract postag features by using the pos_nb function."],"metadata":{"id":"YP8-aZaBn9Oe"}},{"cell_type":"code","metadata":{"collapsed":true,"id":"jciOyuRjgipt","executionInfo":{"status":"ok","timestamp":1644674410418,"user_tz":-60,"elapsed":48,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["from sklearn.feature_extraction import DictVectorizer\n","\n","def extract_instance_features(instance, target_matcher=None,\n","                              feature_types=None):\n","    if feature_types is None:\n","        feature_types = {'ngrams' : True,\n","                        'cgrams' : True,\n","                        'sentiment' : True,\n","                        'nb_pos' : True,\n","                        'target' : True}\n","    # tokenize\n","    tokenized = tokenize(instance)\n","    \n","    # part-of-speech\n","    pos = nltk.pos_tag(tokenized)\n","    \n","    # extract features\n","    features = {}\n","    \n","    # word n-grams\n","    if feature_types['ngrams']:\n","        features.update(word_ngrams(tokenized, 1))\n","        features.update(word_ngrams(tokenized, 2))\n","        features.update(word_ngrams(tokenized, 3))\n","    \n","    # TODO implement bigram and trigram character n-grams features\n","    if feature_types['cgrams']:\n","        features.update(char_ngrams(instance, 2))\n","        features.update(char_ngrams(instance, 3))\n","    \n","    # sentiment\n","    if feature_types['sentiment']:\n","        features.update(nrc_emotions_features(tokenized, nrc_emotions))\n","        features.update(nrc_hashtag_features(tokenized, nrc_ht_sentiments))\n","        features.update(nrc_emoticons_features(tokenized, nrc_em_sentiments))\n","        features.update(hu_liu_sentiment_features(tokenized, hu_liu_sentiments))\n","        features.update(mpqa_polarity_features(tokenized, mpqa_polarities))\n","    \n","    # TODO extract postag features \n","    features.update(pos_nb(pos))\n","    \n","    # target\n","    if feature_types['target'] and target_matcher is not None:\n","        features.update(target_occurs(tokenized, target_matcher))\n","    \n","    return features\n","\n","def extract_features(instances, target_names, \n","                     feature_types=None):\n","    target_dict = generate_target_dict(target_names)\n","    matcher = Match()\n","    matcher.matchinit(target_dict)\n","    features = [extract_instance_features(inst, matcher, feature_types) for inst in instances]\n","    return features"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uXMWb6p8gipu"},"source":["---\n","\n","Example of feature extraction of a single instance."]},{"cell_type":"code","metadata":{"id":"vvpUtXFFgipv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644674410419,"user_tz":-60,"elapsed":48,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"70273845-c1ee-4b7a-8daf-6f1f8461dfa2"},"source":["target = 'Hillary Clinton'\n","target_dict = generate_target_dict([target])\n","matcher = Match()\n","matcher.matchinit(target_dict)\n","\n","print(target_dict)\n","print(training_data['Tweet'].iloc[0])\n","feats = extract_instance_features(training_data['Tweet'].iloc[0], matcher)\n","feats"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["{'Hillary Clinton': 1, 'hillary clinton': 1, 'HillaryClinton': 1, '#HillaryClinton': 1, '@HillaryClinton': 1, 'hillaryclinton': 1, '#hillaryclinton': 1, '@hillaryclinton': 1, 'Hillary': 1, '#Hillary': 1, '@Hillary': 1, '#hillary': 1, '@hillary': 1, 'Clinton': 1, '#Clinton': 1, '@Clinton': 1, '#clinton': 1, '@clinton': 1}\n","@tedcruz And, #HandOverTheServer she wiped clean + 30k deleted emails, explains dereliction of duty/lies re #Benghazi,etc #tcot #SemST\n"]},{"output_type":"execute_result","data":{"text/plain":["{'1wgram:#Benghazi': 1,\n"," '1wgram:#HandOverTheServer': 1,\n"," '1wgram:#SemST': 1,\n"," '1wgram:#tcot': 1,\n"," '1wgram:+': 1,\n"," '1wgram:,': 1,\n"," '1wgram:/': 1,\n"," '1wgram:30k': 1,\n"," '1wgram:@tedcruz': 1,\n"," '1wgram:And': 1,\n"," '1wgram:clean': 1,\n"," '1wgram:deleted': 1,\n"," '1wgram:dereliction': 1,\n"," '1wgram:duty': 1,\n"," '1wgram:emails': 1,\n"," '1wgram:etc': 1,\n"," '1wgram:explains': 1,\n"," '1wgram:lies': 1,\n"," '1wgram:of': 1,\n"," '1wgram:re': 1,\n"," '1wgram:she': 1,\n"," '1wgram:wiped': 1,\n"," '2cgram: _#': 1,\n"," '2cgram: _+': 1,\n"," '2cgram: _3': 1,\n"," '2cgram: _A': 1,\n"," '2cgram: _c': 1,\n"," '2cgram: _d': 1,\n"," '2cgram: _e': 1,\n"," '2cgram: _o': 1,\n"," '2cgram: _r': 1,\n"," '2cgram: _s': 1,\n"," '2cgram: _w': 1,\n"," '2cgram:#_B': 1,\n"," '2cgram:#_H': 1,\n"," '2cgram:#_S': 1,\n"," '2cgram:#_t': 1,\n"," '2cgram:+_ ': 1,\n"," '2cgram:,_ ': 1,\n"," '2cgram:,_e': 1,\n"," '2cgram:/_l': 1,\n"," '2cgram:0_k': 1,\n"," '2cgram:3_0': 1,\n"," '2cgram:@_t': 1,\n"," '2cgram:A_n': 1,\n"," '2cgram:B_e': 1,\n"," '2cgram:H_a': 1,\n"," '2cgram:O_v': 1,\n"," '2cgram:S_T': 1,\n"," '2cgram:S_e': 1,\n"," '2cgram:T_h': 1,\n"," '2cgram:a_i': 1,\n"," '2cgram:a_n': 1,\n"," '2cgram:a_z': 1,\n"," '2cgram:c_ ': 1,\n"," '2cgram:c_l': 1,\n"," '2cgram:c_o': 1,\n"," '2cgram:c_r': 1,\n"," '2cgram:c_t': 1,\n"," '2cgram:d_ ': 1,\n"," '2cgram:d_,': 1,\n"," '2cgram:d_O': 1,\n"," '2cgram:d_c': 1,\n"," '2cgram:d_e': 1,\n"," '2cgram:d_u': 1,\n"," '2cgram:e_ ': 1,\n"," '2cgram:e_S': 1,\n"," '2cgram:e_a': 1,\n"," '2cgram:e_d': 1,\n"," '2cgram:e_l': 1,\n"," '2cgram:e_m': 1,\n"," '2cgram:e_n': 1,\n"," '2cgram:e_r': 1,\n"," '2cgram:e_s': 1,\n"," '2cgram:e_t': 1,\n"," '2cgram:e_x': 1,\n"," '2cgram:f_ ': 1,\n"," '2cgram:g_h': 1,\n"," '2cgram:h_a': 1,\n"," '2cgram:h_e': 1,\n"," '2cgram:i_,': 1,\n"," '2cgram:i_c': 1,\n"," '2cgram:i_e': 1,\n"," '2cgram:i_l': 1,\n"," '2cgram:i_n': 1,\n"," '2cgram:i_o': 1,\n"," '2cgram:i_p': 1,\n"," '2cgram:k_ ': 1,\n"," '2cgram:l_a': 1,\n"," '2cgram:l_e': 1,\n"," '2cgram:l_i': 1,\n"," '2cgram:l_s': 1,\n"," '2cgram:m_S': 1,\n"," '2cgram:m_a': 1,\n"," '2cgram:n_ ': 1,\n"," '2cgram:n_d': 1,\n"," '2cgram:n_g': 1,\n"," '2cgram:n_s': 1,\n"," '2cgram:o_f': 1,\n"," '2cgram:o_n': 1,\n"," '2cgram:o_t': 1,\n"," '2cgram:p_e': 1,\n"," '2cgram:p_l': 1,\n"," '2cgram:r_ ': 1,\n"," '2cgram:r_T': 1,\n"," '2cgram:r_e': 1,\n"," '2cgram:r_u': 1,\n"," '2cgram:r_v': 1,\n"," '2cgram:s_ ': 1,\n"," '2cgram:s_,': 1,\n"," '2cgram:s_h': 1,\n"," '2cgram:t_ ': 1,\n"," '2cgram:t_c': 1,\n"," '2cgram:t_e': 1,\n"," '2cgram:t_i': 1,\n"," '2cgram:t_y': 1,\n"," '2cgram:u_t': 1,\n"," '2cgram:u_z': 1,\n"," '2cgram:v_e': 1,\n"," '2cgram:w_i': 1,\n"," '2cgram:x_p': 1,\n"," '2cgram:y_/': 1,\n"," '2cgram:z_ ': 1,\n"," '2cgram:z_i': 1,\n"," '2wgram:#Benghazi_,': 1,\n"," '2wgram:#HandOverTheServer_she': 1,\n"," '2wgram:#tcot_#SemST': 1,\n"," '2wgram:+_30k': 1,\n"," '2wgram:,_#HandOverTheServer': 1,\n"," '2wgram:,_etc': 1,\n"," '2wgram:,_explains': 1,\n"," '2wgram:/_lies': 1,\n"," '2wgram:30k_deleted': 1,\n"," '2wgram:@tedcruz_And': 1,\n"," '2wgram:And_,': 1,\n"," '2wgram:clean_+': 1,\n"," '2wgram:deleted_emails': 1,\n"," '2wgram:dereliction_of': 1,\n"," '2wgram:duty_/': 1,\n"," '2wgram:emails_,': 1,\n"," '2wgram:etc_#tcot': 1,\n"," '2wgram:explains_dereliction': 1,\n"," '2wgram:lies_re': 1,\n"," '2wgram:of_duty': 1,\n"," '2wgram:re_#Benghazi': 1,\n"," '2wgram:she_wiped': 1,\n"," '2wgram:wiped_clean': 1,\n"," '3cgram: _#_B': 1,\n"," '3cgram: _#_H': 1,\n"," '3cgram: _#_S': 1,\n"," '3cgram: _#_t': 1,\n"," '3cgram: _+_ ': 1,\n"," '3cgram: _3_0': 1,\n"," '3cgram: _A_n': 1,\n"," '3cgram: _c_l': 1,\n"," '3cgram: _d_e': 1,\n"," '3cgram: _d_u': 1,\n"," '3cgram: _e_m': 1,\n"," '3cgram: _e_x': 1,\n"," '3cgram: _o_f': 1,\n"," '3cgram: _r_e': 1,\n"," '3cgram: _s_h': 1,\n"," '3cgram: _w_i': 1,\n"," '3cgram:#_B_e': 1,\n"," '3cgram:#_H_a': 1,\n"," '3cgram:#_S_e': 1,\n"," '3cgram:#_t_c': 1,\n"," '3cgram:+_ _3': 1,\n"," '3cgram:,_ _#': 1,\n"," '3cgram:,_ _e': 1,\n"," '3cgram:,_e_t': 1,\n"," '3cgram:/_l_i': 1,\n"," '3cgram:0_k_ ': 1,\n"," '3cgram:3_0_k': 1,\n"," '3cgram:@_t_e': 1,\n"," '3cgram:A_n_d': 1,\n"," '3cgram:B_e_n': 1,\n"," '3cgram:H_a_n': 1,\n"," '3cgram:O_v_e': 1,\n"," '3cgram:S_e_m': 1,\n"," '3cgram:S_e_r': 1,\n"," '3cgram:T_h_e': 1,\n"," '3cgram:a_i_l': 1,\n"," '3cgram:a_i_n': 1,\n"," '3cgram:a_n_ ': 1,\n"," '3cgram:a_n_d': 1,\n"," '3cgram:a_z_i': 1,\n"," '3cgram:c_ _#': 1,\n"," '3cgram:c_l_e': 1,\n"," '3cgram:c_o_t': 1,\n"," '3cgram:c_r_u': 1,\n"," '3cgram:c_t_i': 1,\n"," '3cgram:d_ _c': 1,\n"," '3cgram:d_ _e': 1,\n"," '3cgram:d_,_ ': 1,\n"," '3cgram:d_O_v': 1,\n"," '3cgram:d_c_r': 1,\n"," '3cgram:d_e_l': 1,\n"," '3cgram:d_e_r': 1,\n"," '3cgram:d_u_t': 1,\n"," '3cgram:e_ _#': 1,\n"," '3cgram:e_ _w': 1,\n"," '3cgram:e_S_e': 1,\n"," '3cgram:e_a_n': 1,\n"," '3cgram:e_d_ ': 1,\n"," '3cgram:e_d_c': 1,\n"," '3cgram:e_l_e': 1,\n"," '3cgram:e_l_i': 1,\n"," '3cgram:e_m_S': 1,\n"," '3cgram:e_m_a': 1,\n"," '3cgram:e_n_g': 1,\n"," '3cgram:e_r_ ': 1,\n"," '3cgram:e_r_T': 1,\n"," '3cgram:e_r_e': 1,\n"," '3cgram:e_r_v': 1,\n"," '3cgram:e_s_ ': 1,\n"," '3cgram:e_t_c': 1,\n"," '3cgram:e_t_e': 1,\n"," '3cgram:e_x_p': 1,\n"," '3cgram:f_ _d': 1,\n"," '3cgram:g_h_a': 1,\n"," '3cgram:h_a_z': 1,\n"," '3cgram:h_e_ ': 1,\n"," '3cgram:h_e_S': 1,\n"," '3cgram:i_,_e': 1,\n"," '3cgram:i_c_t': 1,\n"," '3cgram:i_e_s': 1,\n"," '3cgram:i_l_s': 1,\n"," '3cgram:i_n_s': 1,\n"," '3cgram:i_o_n': 1,\n"," '3cgram:i_p_e': 1,\n"," '3cgram:k_ _d': 1,\n"," '3cgram:l_a_i': 1,\n"," '3cgram:l_e_a': 1,\n"," '3cgram:l_e_t': 1,\n"," '3cgram:l_i_c': 1,\n"," '3cgram:l_i_e': 1,\n"," '3cgram:l_s_,': 1,\n"," '3cgram:m_S_T': 1,\n"," '3cgram:m_a_i': 1,\n"," '3cgram:n_ _+': 1,\n"," '3cgram:n_ _o': 1,\n"," '3cgram:n_d_,': 1,\n"," '3cgram:n_d_O': 1,\n"," '3cgram:n_g_h': 1,\n"," '3cgram:n_s_ ': 1,\n"," '3cgram:o_f_ ': 1,\n"," '3cgram:o_n_ ': 1,\n"," '3cgram:o_t_ ': 1,\n"," '3cgram:p_e_d': 1,\n"," '3cgram:p_l_a': 1,\n"," '3cgram:r_ _s': 1,\n"," '3cgram:r_T_h': 1,\n"," '3cgram:r_e_ ': 1,\n"," '3cgram:r_e_l': 1,\n"," '3cgram:r_u_z': 1,\n"," '3cgram:r_v_e': 1,\n"," '3cgram:s_ _d': 1,\n"," '3cgram:s_ _r': 1,\n"," '3cgram:s_,_ ': 1,\n"," '3cgram:s_h_e': 1,\n"," '3cgram:t_ _#': 1,\n"," '3cgram:t_c_ ': 1,\n"," '3cgram:t_c_o': 1,\n"," '3cgram:t_e_d': 1,\n"," '3cgram:t_i_o': 1,\n"," '3cgram:t_y_/': 1,\n"," '3cgram:u_t_y': 1,\n"," '3cgram:u_z_ ': 1,\n"," '3cgram:v_e_r': 1,\n"," '3cgram:w_i_p': 1,\n"," '3cgram:x_p_l': 1,\n"," '3cgram:y_/_l': 1,\n"," '3cgram:z_ _A': 1,\n"," '3cgram:z_i_,': 1,\n"," '3wgram:#Benghazi_,_etc': 1,\n"," '3wgram:#HandOverTheServer_she_wiped': 1,\n"," '3wgram:+_30k_deleted': 1,\n"," '3wgram:,_#HandOverTheServer_she': 1,\n"," '3wgram:,_etc_#tcot': 1,\n"," '3wgram:,_explains_dereliction': 1,\n"," '3wgram:/_lies_re': 1,\n"," '3wgram:30k_deleted_emails': 1,\n"," '3wgram:@tedcruz_And_,': 1,\n"," '3wgram:And_,_#HandOverTheServer': 1,\n"," '3wgram:clean_+_30k': 1,\n"," '3wgram:deleted_emails_,': 1,\n"," '3wgram:dereliction_of_duty': 1,\n"," '3wgram:duty_/_lies': 1,\n"," '3wgram:emails_,_explains': 1,\n"," '3wgram:etc_#tcot_#SemST': 1,\n"," '3wgram:explains_dereliction_of': 1,\n"," '3wgram:lies_re_#Benghazi': 1,\n"," '3wgram:of_duty_/': 1,\n"," '3wgram:re_#Benghazi_,': 1,\n"," '3wgram:she_wiped_clean': 1,\n"," '3wgram:wiped_clean_+': 1,\n"," 'hu_liu:clean:positive': 1,\n"," 'hu_liu:lies:negative': 1,\n"," 'mpqa:clean:positive': 1,\n"," 'mpqa:duty:neutral': 1,\n"," 'mpqa:lies:negative': 1,\n"," 'nrc_emc:#tcot': '1.481',\n"," 'nrc_emc:+': '0.265',\n"," 'nrc_emc:,': '0.2',\n"," 'nrc_emc:/': '0.396',\n"," 'nrc_emc:30k': '-0.798',\n"," 'nrc_emc:clean': '-0.262',\n"," 'nrc_emc:deleted': '-1.318',\n"," 'nrc_emc:duty': '-0.564',\n"," 'nrc_emc:emails': '-0.049',\n"," 'nrc_emc:etc': '0.317',\n"," 'nrc_emc:explains': '0.58',\n"," 'nrc_emc:lies': '-0.029',\n"," 'nrc_emc:of': '0.031',\n"," 'nrc_emc:re': '0.551',\n"," 'nrc_emc:she': '-0.338',\n"," 'nrc_emc:wiped': '-1.019',\n"," 'nrc_emo:clean:joy': 1,\n"," 'nrc_emo:clean:positive': 1,\n"," 'nrc_emo:clean:trust': 1,\n"," 'nrc_ht:#tcot': '-1.554',\n"," 'nrc_ht:+': '1.275',\n"," 'nrc_ht:,': '0.271',\n"," 'nrc_ht:/': '-0.91',\n"," 'nrc_ht:30k': '0.072',\n"," 'nrc_ht:@tedcruz': '-4.999',\n"," 'nrc_ht:clean': '0.007',\n"," 'nrc_ht:deleted': '-0.672',\n"," 'nrc_ht:duty': '-0.3',\n"," 'nrc_ht:emails': '-0.424',\n"," 'nrc_ht:etc': '-0.582',\n"," 'nrc_ht:explains': '-0.7',\n"," 'nrc_ht:lies': '-1.137',\n"," 'nrc_ht:of': '0.141',\n"," 'nrc_ht:re': '-0.361',\n"," 'nrc_ht:she': '-0.314',\n"," 'nrc_ht:wiped': '-0.383',\n"," 'pos_nb:,': 3,\n"," 'pos_nb:CC': 1,\n"," 'pos_nb:CD': 2,\n"," 'pos_nb:FW': 1,\n"," 'pos_nb:IN': 1,\n"," 'pos_nb:JJ': 1,\n"," 'pos_nb:NN': 6,\n"," 'pos_nb:NNP': 2,\n"," 'pos_nb:NNS': 1,\n"," 'pos_nb:PRP': 1,\n"," 'pos_nb:VB': 1,\n"," 'pos_nb:VBD': 2,\n"," 'pos_nb:VBZ': 2,\n"," 'target:false': 1}"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"0cLRTIEFgipv"},"source":["## Experiments\n","\n","- **Classifier**: Class weigthed Linear SVM. No previous feature scaling is done (previous experiments showed that takes longer learning and performance is lower).\n","- **Experiments**: We'll run experiments in a in-target scenario running independent experiments in each of 5 targets.\n","    - We'll run ablation test on feature types.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oWo8r43xgipw"},"source":["<a id='intarget'></a>\n","### In-target scenario\n","\n","This section is organized in two parts. First, we'll run a linear SVM with all the features and explore the effect of C when using discrete binary features. Second, we'll run an ablation test to measure the impact of each feature type over the whole system.\n","\n","Finally, we'll compare all the results and draw some conclusions.\n","\n","**Results on ablation test**\n","\n","- overall, when we remove nb_pos feature the system improve about 1 point.\n","- character grams are the ones that contribute more in the system. This might because they are the predominant type of features by far.\n","- The rest contributes little or nothing regarding the whole system."]},{"cell_type":"markdown","metadata":{"id":"NE7OHescgipx"},"source":["### Experiments with whole set of features"]},{"cell_type":"code","metadata":{"id":"db2IYB-cgipx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644674457085,"user_tz":-60,"elapsed":46670,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"a34c728f-0412-4035-9d88-39ccf7d8c285"},"source":["%%capture --no-stdout\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, make_scorer\n","from sklearn.model_selection import cross_validate\n","from sklearn.feature_extraction import DictVectorizer\n","\n","\n","# Run experiments: preprocess data and glove\n","results = {}\n","for target in targets:\n","    results[target] = {'svm': {'str':'', 'f1':0.0}}\n","\n","scoring = {'accuracy' : 'accuracy',\n","           'f1': 'f1_macro',\n","           'precision':'precision_macro',\n","           'recall' : 'recall_macro'}\n","\n","xval = 5\n","# feature type: ALL\n","feature_types = {'ngrams' : True,\n","                 'cgrams' : True,\n","                 'sentiment' : True,\n","                 'nb_pos' : True,\n","                 'target' : True}\n","\n","for target in targets:\n","    print('Running experiments for \"{}\"'.format(target))\n","    \n","    training_texts, training_labels = data_as_numpy(training_data[training_data['Target'] == target])\n","    training_features = extract_features(training_texts, [target], feature_types)\n"," \n","    #vectorize\n","    vec = DictVectorizer()\n","    train_x = vec.fit_transform(training_features).toarray()\n","    train_y, labels = encode_labels(training_labels)\n","    \n","    print('Training shape: {}'.format(train_x.shape))\n","    \n","    C = [0.1, 0.5, 1, 10]\n","    for c in C:\n","        linear_svm = Pipeline([\n","            #(\"scaler\", StandardScaler()),\n","            (\"lsvc\", LinearSVC(C=c, class_weight='balanced'))\n","        ])\n","        scores = cross_validate(linear_svm, train_x, train_y, cv=xval, scoring=scoring, return_train_score=False)\n","        f1 = np.mean(scores['test_f1'])\n","        print(\"[LinearSVM] C=%f | f1=%f\" %(c,f1))\n","        if f1 > results[target]['svm']['f1']:\n","            results[target]['svm']['f1'] = f1\n","            results[target]['svm']['str'] = \"[LinearSVM] C=%f | f1=%f\" %(c,f1)\n","            results[target]['svm']['C'] = c\n","            results[target]['svm']['scores'] = scores\n","    print()"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Running experiments for \"Atheism\"\n","Training shape: (513, 34683)\n","[LinearSVM] C=0.100000 | f1=0.613103\n","[LinearSVM] C=0.500000 | f1=0.610663\n","[LinearSVM] C=1.000000 | f1=0.610663\n","[LinearSVM] C=10.000000 | f1=0.610663\n","\n","Running experiments for \"Hillary Clinton\"\n","Training shape: (689, 41332)\n","[LinearSVM] C=0.100000 | f1=0.598669\n","[LinearSVM] C=0.500000 | f1=0.596581\n","[LinearSVM] C=1.000000 | f1=0.596581\n","[LinearSVM] C=10.000000 | f1=0.596581\n","\n","Running experiments for \"Legalization of Abortion\"\n","Training shape: (653, 39874)\n","[LinearSVM] C=0.100000 | f1=0.604665\n","[LinearSVM] C=0.500000 | f1=0.598369\n","[LinearSVM] C=1.000000 | f1=0.598369\n","[LinearSVM] C=10.000000 | f1=0.598369\n","\n","Running experiments for \"Feminist Movement\"\n","Training shape: (664, 42514)\n","[LinearSVM] C=0.100000 | f1=0.530827\n","[LinearSVM] C=0.500000 | f1=0.528095\n","[LinearSVM] C=1.000000 | f1=0.528095\n","[LinearSVM] C=10.000000 | f1=0.528095\n","\n","Running experiments for \"Climate Change is a Real Concern\"\n","Training shape: (395, 28313)\n","[LinearSVM] C=0.100000 | f1=0.568759\n","[LinearSVM] C=0.500000 | f1=0.568759\n","[LinearSVM] C=1.000000 | f1=0.568759\n","[LinearSVM] C=10.000000 | f1=0.568759\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"tQp22wTCgipy"},"source":["### Ablation tests"]},{"cell_type":"code","metadata":{"id":"vv6OvMWigipy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644674619057,"user_tz":-60,"elapsed":161995,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"063da9cf-10f7-47d7-ca0f-6156d6d955fe"},"source":["%%capture --no-stdout\n","# run ablation test over feature types\n","feature_type_names = feature_types.keys()\n","for target in targets:\n","    for feature_type in feature_type_names:\n","        results[target]['svm-'+feature_type] = {'str':'', 'f1':0.0}\n","\n","def init_feature_type():\n","    return {'ngrams' : True,'cgrams' : True,'sentiment' : True,'nb_pos' : True, 'target' : True}\n","\n","scoring = {'accuracy' : 'accuracy',\n","           'f1': 'f1_macro',\n","           'precision':'precision_macro',\n","           'recall' : 'recall_macro'}\n","\n","xval = 5\n","for target in targets:\n","    print('Running experiments for \"{}\"'.format(target))\n","    training_texts, training_labels = data_as_numpy(training_data[training_data['Target'] == target])\n","    \n","    # run ablation in target\n","    for feature_type in feature_type_names:\n","        feature_types = init_feature_type()\n","        # deactivate feature type\n","        model_name = 'svm-'+feature_type\n","        feature_types[feature_type] = False\n","        training_features = extract_features(training_texts, [target], feature_types)\n"," \n","        #vectorize\n","        vec = DictVectorizer()\n","        train_x = vec.fit_transform(training_features).toarray()\n","        train_y, labels = encode_labels(training_labels)\n","        \n","        print('Removed feature type: {}'.format(feature_type))\n","        print('Training shape: {}'.format(train_x.shape))\n","\n","        C = [0.1, 0.5, 1, 10]\n","        for c in C:\n","            linear_svm = Pipeline([\n","                #(\"scaler\", StandardScaler()),\n","                (\"lsvc\", LinearSVC(C=c, class_weight='balanced'))\n","            ])\n","            scores = cross_validate(linear_svm, train_x, train_y, cv=xval, scoring=scoring, return_train_score=False)\n","            f1 = np.mean(scores['test_f1'])\n","            print(\"[LinearSVM] C=%f | f1=%f\" %(c,f1))\n","            if f1 > results[target][model_name]['f1']:\n","                results[target][model_name]['f1'] = f1\n","                results[target][model_name]['str'] = \"[LinearSVM] C=%f | f1=%f\" %(c,f1)\n","                results[target][model_name]['C'] = c\n","                results[target][model_name]['scores'] = scores\n","        print()"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Running experiments for \"Atheism\"\n","Removed feature type: ngrams\n","Training shape: (513, 14393)\n","[LinearSVM] C=0.100000 | f1=0.604565\n","[LinearSVM] C=0.500000 | f1=0.598419\n","[LinearSVM] C=1.000000 | f1=0.598419\n","[LinearSVM] C=10.000000 | f1=0.597084\n","\n","Removed feature type: cgrams\n","Training shape: (513, 25447)\n","[LinearSVM] C=0.100000 | f1=0.548421\n","[LinearSVM] C=0.500000 | f1=0.535866\n","[LinearSVM] C=1.000000 | f1=0.535866\n","[LinearSVM] C=10.000000 | f1=0.535866\n","\n","Removed feature type: sentiment\n","Training shape: (513, 29569)\n","[LinearSVM] C=0.100000 | f1=0.604829\n","[LinearSVM] C=0.500000 | f1=0.608222\n","[LinearSVM] C=1.000000 | f1=0.608222\n","[LinearSVM] C=10.000000 | f1=0.608222\n","\n","Removed feature type: nb_pos\n","Training shape: (513, 34683)\n","[LinearSVM] C=0.100000 | f1=0.613103\n","[LinearSVM] C=0.500000 | f1=0.610663\n","[LinearSVM] C=1.000000 | f1=0.610663\n","[LinearSVM] C=10.000000 | f1=0.610663\n","\n","Removed feature type: target\n","Training shape: (513, 34681)\n","[LinearSVM] C=0.100000 | f1=0.613103\n","[LinearSVM] C=0.500000 | f1=0.610663\n","[LinearSVM] C=1.000000 | f1=0.610663\n","[LinearSVM] C=10.000000 | f1=0.610663\n","\n","Running experiments for \"Hillary Clinton\"\n","Removed feature type: ngrams\n","Training shape: (689, 17256)\n","[LinearSVM] C=0.100000 | f1=0.597064\n","[LinearSVM] C=0.500000 | f1=0.589589\n","[LinearSVM] C=1.000000 | f1=0.586834\n","[LinearSVM] C=10.000000 | f1=0.586834\n","\n","Removed feature type: cgrams\n","Training shape: (689, 29596)\n","[LinearSVM] C=0.100000 | f1=0.536649\n","[LinearSVM] C=0.500000 | f1=0.532319\n","[LinearSVM] C=1.000000 | f1=0.523192\n","[LinearSVM] C=10.000000 | f1=0.526454\n","\n","Removed feature type: sentiment\n","Training shape: (689, 35858)\n","[LinearSVM] C=0.100000 | f1=0.591254\n","[LinearSVM] C=0.500000 | f1=0.592283\n","[LinearSVM] C=1.000000 | f1=0.592283\n","[LinearSVM] C=10.000000 | f1=0.592283\n","\n","Removed feature type: nb_pos\n","Training shape: (689, 41332)\n","[LinearSVM] C=0.100000 | f1=0.598669\n","[LinearSVM] C=0.500000 | f1=0.596581\n","[LinearSVM] C=1.000000 | f1=0.596581\n","[LinearSVM] C=10.000000 | f1=0.596581\n","\n","Removed feature type: target\n","Training shape: (689, 41330)\n","[LinearSVM] C=0.100000 | f1=0.599157\n","[LinearSVM] C=0.500000 | f1=0.597509\n","[LinearSVM] C=1.000000 | f1=0.597509\n","[LinearSVM] C=10.000000 | f1=0.597509\n","\n","Running experiments for \"Legalization of Abortion\"\n","Removed feature type: ngrams\n","Training shape: (653, 15933)\n","[LinearSVM] C=0.100000 | f1=0.599122\n","[LinearSVM] C=0.500000 | f1=0.590981\n","[LinearSVM] C=1.000000 | f1=0.589325\n","[LinearSVM] C=10.000000 | f1=0.589718\n","\n","Removed feature type: cgrams\n","Training shape: (653, 29231)\n","[LinearSVM] C=0.100000 | f1=0.541251\n","[LinearSVM] C=0.500000 | f1=0.534337\n","[LinearSVM] C=1.000000 | f1=0.535605\n","[LinearSVM] C=10.000000 | f1=0.536692\n","\n","Removed feature type: sentiment\n","Training shape: (653, 34629)\n","[LinearSVM] C=0.100000 | f1=0.600099\n","[LinearSVM] C=0.500000 | f1=0.597722\n","[LinearSVM] C=1.000000 | f1=0.596603\n","[LinearSVM] C=10.000000 | f1=0.595378\n","\n","Removed feature type: nb_pos\n","Training shape: (653, 39874)\n","[LinearSVM] C=0.100000 | f1=0.604665\n","[LinearSVM] C=0.500000 | f1=0.598369\n","[LinearSVM] C=1.000000 | f1=0.598369\n","[LinearSVM] C=10.000000 | f1=0.598369\n","\n","Removed feature type: target\n","Training shape: (653, 39872)\n","[LinearSVM] C=0.100000 | f1=0.605779\n","[LinearSVM] C=0.500000 | f1=0.596588\n","[LinearSVM] C=1.000000 | f1=0.596588\n","[LinearSVM] C=10.000000 | f1=0.596588\n","\n","Running experiments for \"Feminist Movement\"\n","Removed feature type: ngrams\n","Training shape: (664, 17420)\n","[LinearSVM] C=0.100000 | f1=0.520549\n","[LinearSVM] C=0.500000 | f1=0.517585\n","[LinearSVM] C=1.000000 | f1=0.517585\n","[LinearSVM] C=10.000000 | f1=0.517585\n","\n","Removed feature type: cgrams\n","Training shape: (664, 31553)\n","[LinearSVM] C=0.100000 | f1=0.422438\n","[LinearSVM] C=0.500000 | f1=0.421072\n","[LinearSVM] C=1.000000 | f1=0.420049\n","[LinearSVM] C=10.000000 | f1=0.425588\n","\n","Removed feature type: sentiment\n","Training shape: (664, 36100)\n","[LinearSVM] C=0.100000 | f1=0.522695\n","[LinearSVM] C=0.500000 | f1=0.520508\n","[LinearSVM] C=1.000000 | f1=0.520508\n","[LinearSVM] C=10.000000 | f1=0.518710\n","\n","Removed feature type: nb_pos\n","Training shape: (664, 42514)\n","[LinearSVM] C=0.100000 | f1=0.530827\n","[LinearSVM] C=0.500000 | f1=0.528095\n","[LinearSVM] C=1.000000 | f1=0.528095\n","[LinearSVM] C=10.000000 | f1=0.528095\n","\n","Removed feature type: target\n","Training shape: (664, 42512)\n","[LinearSVM] C=0.100000 | f1=0.532535\n","[LinearSVM] C=0.500000 | f1=0.529539\n","[LinearSVM] C=1.000000 | f1=0.529539\n","[LinearSVM] C=10.000000 | f1=0.529539\n","\n","Running experiments for \"Climate Change is a Real Concern\"\n","Removed feature type: ngrams\n","Training shape: (395, 13218)\n","[LinearSVM] C=0.100000 | f1=0.563089\n","[LinearSVM] C=0.500000 | f1=0.561549\n","[LinearSVM] C=1.000000 | f1=0.561549\n","[LinearSVM] C=10.000000 | f1=0.561549\n","\n","Removed feature type: cgrams\n","Training shape: (395, 19186)\n","[LinearSVM] C=0.100000 | f1=0.525366\n","[LinearSVM] C=0.500000 | f1=0.530505\n","[LinearSVM] C=1.000000 | f1=0.530505\n","[LinearSVM] C=10.000000 | f1=0.530505\n","\n","Removed feature type: sentiment\n","Training shape: (395, 24266)\n","[LinearSVM] C=0.100000 | f1=0.565229\n","[LinearSVM] C=0.500000 | f1=0.565229\n","[LinearSVM] C=1.000000 | f1=0.565229\n","[LinearSVM] C=10.000000 | f1=0.565229\n","\n","Removed feature type: nb_pos\n","Training shape: (395, 28313)\n","[LinearSVM] C=0.100000 | f1=0.568759\n","[LinearSVM] C=0.500000 | f1=0.568759\n","[LinearSVM] C=1.000000 | f1=0.568759\n","[LinearSVM] C=10.000000 | f1=0.568759\n","\n","Removed feature type: target\n","Training shape: (395, 28311)\n","[LinearSVM] C=0.100000 | f1=0.568759\n","[LinearSVM] C=0.500000 | f1=0.568759\n","[LinearSVM] C=1.000000 | f1=0.568759\n","[LinearSVM] C=10.000000 | f1=0.568759\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"6bjq6Jmin1Xi"},"source":["# ASSIGNMENT 3\n","\n","Create a table to show the ablation results in a table such as the one shown below.\n","\n","+ TODO Create table\n","+ TODO Sort table to show the ablation results ordered by target, fscore and model"]},{"cell_type":"code","source":["#TODO sort table to show the ablation results ordered by target, fscore and model.\n","from collections import defaultdict\n","data_results = defaultdict(list)\n","for target, values in results.items():\n","    for model, params in values.items():\n","        data_results[\"target\"].append(target)\n","        data_results[\"model\"].append(model)\n","        data_results[\"accuracy\"].append(np.mean(params[\"scores\"][\"test_accuracy\"]))\n","        data_results[\"precision\"].append(np.mean(params[\"scores\"][\"test_precision\"]))\n","        data_results[\"recall\"].append(np.mean(params[\"scores\"][\"test_recall\"]))\n","        data_results[\"fscore\"].append(np.mean(params[\"scores\"][\"test_f1\"]))\n","        data_results[\"par_name\"].append(\"C\")\n","        data_results[\"par_value\"].append(params[\"C\"])\n","\n","df_results = pd.DataFrame(data_results)\n","df_results.sort_values(by=['target', 'fscore', 'model'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":990},"id":"ehdOzstNJId9","executionInfo":{"status":"ok","timestamp":1644674619058,"user_tz":-60,"elapsed":24,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"4bbf6299-91d3-456b-b7dc-46f861734aed"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-8d6e5a78-07dc-4e02-8008-837fa1d31fd8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","      <th>model</th>\n","      <th>accuracy</th>\n","      <th>precision</th>\n","      <th>recall</th>\n","      <th>fscore</th>\n","      <th>par_name</th>\n","      <th>par_value</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2</th>\n","      <td>Atheism</td>\n","      <td>svm-cgrams</td>\n","      <td>0.672568</td>\n","      <td>0.635896</td>\n","      <td>0.532931</td>\n","      <td>0.548421</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Atheism</td>\n","      <td>svm-ngrams</td>\n","      <td>0.691986</td>\n","      <td>0.675519</td>\n","      <td>0.588384</td>\n","      <td>0.604565</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Atheism</td>\n","      <td>svm-sentiment</td>\n","      <td>0.695869</td>\n","      <td>0.677185</td>\n","      <td>0.593439</td>\n","      <td>0.608222</td>\n","      <td>C</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>Atheism</td>\n","      <td>svm</td>\n","      <td>0.703674</td>\n","      <td>0.692320</td>\n","      <td>0.596236</td>\n","      <td>0.613103</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Atheism</td>\n","      <td>svm-nb_pos</td>\n","      <td>0.703674</td>\n","      <td>0.692320</td>\n","      <td>0.596236</td>\n","      <td>0.613103</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Atheism</td>\n","      <td>svm-target</td>\n","      <td>0.703674</td>\n","      <td>0.692320</td>\n","      <td>0.596236</td>\n","      <td>0.613103</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>Climate Change is a Real Concern</td>\n","      <td>svm-cgrams</td>\n","      <td>0.663291</td>\n","      <td>0.573037</td>\n","      <td>0.518366</td>\n","      <td>0.530505</td>\n","      <td>C</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>Climate Change is a Real Concern</td>\n","      <td>svm-ngrams</td>\n","      <td>0.711392</td>\n","      <td>0.607411</td>\n","      <td>0.551402</td>\n","      <td>0.563089</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>Climate Change is a Real Concern</td>\n","      <td>svm-sentiment</td>\n","      <td>0.713924</td>\n","      <td>0.608430</td>\n","      <td>0.553303</td>\n","      <td>0.565229</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>Climate Change is a Real Concern</td>\n","      <td>svm</td>\n","      <td>0.718987</td>\n","      <td>0.611924</td>\n","      <td>0.556911</td>\n","      <td>0.568759</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>Climate Change is a Real Concern</td>\n","      <td>svm-nb_pos</td>\n","      <td>0.718987</td>\n","      <td>0.611924</td>\n","      <td>0.556911</td>\n","      <td>0.568759</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>Climate Change is a Real Concern</td>\n","      <td>svm-target</td>\n","      <td>0.718987</td>\n","      <td>0.611924</td>\n","      <td>0.556911</td>\n","      <td>0.568759</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>Feminist Movement</td>\n","      <td>svm-cgrams</td>\n","      <td>0.500080</td>\n","      <td>0.449591</td>\n","      <td>0.424500</td>\n","      <td>0.425588</td>\n","      <td>C</td>\n","      <td>10.0</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Feminist Movement</td>\n","      <td>svm-ngrams</td>\n","      <td>0.555799</td>\n","      <td>0.532859</td>\n","      <td>0.517776</td>\n","      <td>0.520549</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>Feminist Movement</td>\n","      <td>svm-sentiment</td>\n","      <td>0.563340</td>\n","      <td>0.540220</td>\n","      <td>0.518038</td>\n","      <td>0.522695</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Feminist Movement</td>\n","      <td>svm</td>\n","      <td>0.566382</td>\n","      <td>0.546165</td>\n","      <td>0.525487</td>\n","      <td>0.530827</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>Feminist Movement</td>\n","      <td>svm-nb_pos</td>\n","      <td>0.566382</td>\n","      <td>0.546165</td>\n","      <td>0.525487</td>\n","      <td>0.530827</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>Feminist Movement</td>\n","      <td>svm-target</td>\n","      <td>0.567886</td>\n","      <td>0.547722</td>\n","      <td>0.527074</td>\n","      <td>0.532535</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Hillary Clinton</td>\n","      <td>svm-cgrams</td>\n","      <td>0.611055</td>\n","      <td>0.592867</td>\n","      <td>0.520180</td>\n","      <td>0.536649</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Hillary Clinton</td>\n","      <td>svm-sentiment</td>\n","      <td>0.663281</td>\n","      <td>0.635775</td>\n","      <td>0.585333</td>\n","      <td>0.592283</td>\n","      <td>C</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Hillary Clinton</td>\n","      <td>svm-ngrams</td>\n","      <td>0.658923</td>\n","      <td>0.628946</td>\n","      <td>0.591643</td>\n","      <td>0.597064</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Hillary Clinton</td>\n","      <td>svm</td>\n","      <td>0.666180</td>\n","      <td>0.641863</td>\n","      <td>0.592102</td>\n","      <td>0.598669</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Hillary Clinton</td>\n","      <td>svm-nb_pos</td>\n","      <td>0.666180</td>\n","      <td>0.641863</td>\n","      <td>0.592102</td>\n","      <td>0.598669</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Hillary Clinton</td>\n","      <td>svm-target</td>\n","      <td>0.667640</td>\n","      <td>0.643432</td>\n","      <td>0.591948</td>\n","      <td>0.599157</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>Legalization of Abortion</td>\n","      <td>svm-cgrams</td>\n","      <td>0.598661</td>\n","      <td>0.575281</td>\n","      <td>0.529374</td>\n","      <td>0.541251</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>Legalization of Abortion</td>\n","      <td>svm-ngrams</td>\n","      <td>0.636841</td>\n","      <td>0.622220</td>\n","      <td>0.592357</td>\n","      <td>0.599122</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Legalization of Abortion</td>\n","      <td>svm-sentiment</td>\n","      <td>0.642959</td>\n","      <td>0.631439</td>\n","      <td>0.590742</td>\n","      <td>0.600099</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Legalization of Abortion</td>\n","      <td>svm</td>\n","      <td>0.642983</td>\n","      <td>0.634373</td>\n","      <td>0.596351</td>\n","      <td>0.604665</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Legalization of Abortion</td>\n","      <td>svm-nb_pos</td>\n","      <td>0.642983</td>\n","      <td>0.634373</td>\n","      <td>0.596351</td>\n","      <td>0.604665</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>Legalization of Abortion</td>\n","      <td>svm-target</td>\n","      <td>0.644521</td>\n","      <td>0.635693</td>\n","      <td>0.596324</td>\n","      <td>0.605779</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d6e5a78-07dc-4e02-8008-837fa1d31fd8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8d6e5a78-07dc-4e02-8008-837fa1d31fd8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8d6e5a78-07dc-4e02-8008-837fa1d31fd8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                              target          model  ...  par_name  par_value\n","2                            Atheism     svm-cgrams  ...         C        0.1\n","1                            Atheism     svm-ngrams  ...         C        0.1\n","3                            Atheism  svm-sentiment  ...         C        0.5\n","0                            Atheism            svm  ...         C        0.1\n","4                            Atheism     svm-nb_pos  ...         C        0.1\n","5                            Atheism     svm-target  ...         C        0.1\n","26  Climate Change is a Real Concern     svm-cgrams  ...         C        0.5\n","25  Climate Change is a Real Concern     svm-ngrams  ...         C        0.1\n","27  Climate Change is a Real Concern  svm-sentiment  ...         C        0.1\n","24  Climate Change is a Real Concern            svm  ...         C        0.1\n","28  Climate Change is a Real Concern     svm-nb_pos  ...         C        0.1\n","29  Climate Change is a Real Concern     svm-target  ...         C        0.1\n","20                 Feminist Movement     svm-cgrams  ...         C       10.0\n","19                 Feminist Movement     svm-ngrams  ...         C        0.1\n","21                 Feminist Movement  svm-sentiment  ...         C        0.1\n","18                 Feminist Movement            svm  ...         C        0.1\n","22                 Feminist Movement     svm-nb_pos  ...         C        0.1\n","23                 Feminist Movement     svm-target  ...         C        0.1\n","8                    Hillary Clinton     svm-cgrams  ...         C        0.1\n","9                    Hillary Clinton  svm-sentiment  ...         C        0.5\n","7                    Hillary Clinton     svm-ngrams  ...         C        0.1\n","6                    Hillary Clinton            svm  ...         C        0.1\n","10                   Hillary Clinton     svm-nb_pos  ...         C        0.1\n","11                   Hillary Clinton     svm-target  ...         C        0.1\n","14          Legalization of Abortion     svm-cgrams  ...         C        0.1\n","13          Legalization of Abortion     svm-ngrams  ...         C        0.1\n","15          Legalization of Abortion  svm-sentiment  ...         C        0.1\n","12          Legalization of Abortion            svm  ...         C        0.1\n","16          Legalization of Abortion     svm-nb_pos  ...         C        0.1\n","17          Legalization of Abortion     svm-target  ...         C        0.1\n","\n","[30 rows x 8 columns]"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"9uNraGvBgiqE"},"source":["# (BONUS) ASSIGNMENT 4: Evaluation on Test Dataset\n","\n","Evaluate the models in Task A: **in-target scenario**. Train model using whole training set and best C value of each target. Evaluate on the test set.\n","  + **HINT**: Look at the CV code to iterate over the targets to train and evaluate.\n","\n","**Use both official scorer and sklearn API for evaluation**.\n","  + Hint: Look at the evaluation in the Text Classifier training with Spacy."]},{"cell_type":"markdown","metadata":{"id":"EOPx-4akgiqE"},"source":["### Task A: in-target scenario\n","\n","- Question: Do we obtain comparable results with respect to Mohammad et al. 2016? Yes, results are similar, a bit lower in our case.\n","- Write a table to summarize the results obtained.\n","\n"]},{"cell_type":"code","source":["%%capture --no-stdout\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, make_scorer, classification_report\n","from sklearn.model_selection import cross_validate\n","from sklearn.feature_extraction import DictVectorizer\n","\n","\n","# Run experiments: preprocess data and glove\n","results = {}\n","\n","# feature type: ALL\n","feature_types = {'ngrams' : True,\n","                 'cgrams' : True,\n","                 'sentiment' : True,\n","                 'nb_pos' : True,\n","                 'target' : True}\n","\n","for target in targets:\n","    print('Running experiments for \"{}\"'.format(target))\n","    \n","    training_texts, training_labels = data_as_numpy(training_data[training_data['Target'] == target])\n","    training_features = extract_features(training_texts, [target], feature_types)\n","\n","    test_texts, test_labels = data_as_numpy(test_data[test_data['Target'] == target])\n","    test_features = extract_features(test_texts, [target], feature_types)\n"," \n","    #vectorize\n","    vec = DictVectorizer()\n","    train_x = vec.fit_transform(training_features).toarray()\n","    train_y, labels = encode_labels(training_labels)\n","\n","    test_x = vec.transform(test_features).toarray()\n","    test_y, labels = encode_labels(test_labels)\n","    \n","    print('Training shape: {}'.format(train_x.shape))\n","    \n","    c = 0.1\n","    linear_svm = Pipeline([\n","        #(\"scaler\", StandardScaler()),\n","        (\"lsvc\", LinearSVC(C=c, class_weight='balanced'))\n","    ])\n","\n","    print(\"[LinearSVM] C=%f\" %(c))\n","\n","    linear_svm.fit(train_x, train_y)\n","    preds = linear_svm.predict(test_x)\n","\n","    preds_str = []\n","    for pred in preds:\n","        if pred == 0:\n","            preds_str.append(\"AGAINST\")\n","        if pred == 1:\n","            preds_str.append(\"FAVOR\")\n","        if pred == 2:\n","            preds_str.append(\"NONE\")\n","\n","    report = classification_report(labels, preds_str, labels=['AGAINST', 'FAVOR'])\n","    print(report)\n","\n","    report = classification_report(labels, preds_str, labels=['AGAINST', 'FAVOR'], output_dict=True)\n","    results[target] = report\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XuUC0_X3DmXz","executionInfo":{"status":"ok","timestamp":1644676338352,"user_tz":-60,"elapsed":19086,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"2df7f99a-b889-40a0-e4bd-7e29863328a6"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Running experiments for \"Atheism\"\n","Training shape: (513, 34683)\n","[LinearSVM] C=0.100000\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.85      0.77      0.81       160\n","       FAVOR       0.47      0.50      0.48        32\n","\n","   micro avg       0.78      0.72      0.75       192\n","   macro avg       0.66      0.63      0.65       192\n","weighted avg       0.79      0.72      0.76       192\n","\n","\n","Running experiments for \"Hillary Clinton\"\n","Training shape: (689, 41332)\n","[LinearSVM] C=0.100000\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.71      0.82      0.76       172\n","       FAVOR       0.54      0.33      0.41        45\n","\n","   micro avg       0.69      0.72      0.70       217\n","   macro avg       0.62      0.58      0.59       217\n","weighted avg       0.68      0.72      0.69       217\n","\n","\n","Running experiments for \"Legalization of Abortion\"\n","Training shape: (653, 39874)\n","[LinearSVM] C=0.100000\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.81      0.67      0.73       189\n","       FAVOR       0.49      0.59      0.53        46\n","\n","   micro avg       0.73      0.65      0.69       235\n","   macro avg       0.65      0.63      0.63       235\n","weighted avg       0.75      0.65      0.69       235\n","\n","\n","Running experiments for \"Feminist Movement\"\n","Training shape: (664, 42514)\n","[LinearSVM] C=0.100000\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.77      0.64      0.70       183\n","       FAVOR       0.39      0.64      0.48        58\n","\n","   micro avg       0.62      0.64      0.63       241\n","   macro avg       0.58      0.64      0.59       241\n","weighted avg       0.68      0.64      0.65       241\n","\n","\n","Running experiments for \"Climate Change is a Real Concern\"\n","Training shape: (395, 28313)\n","[LinearSVM] C=0.100000\n","              precision    recall  f1-score   support\n","\n","     AGAINST       0.00      0.00      0.00        11\n","       FAVOR       0.81      0.85      0.83       123\n","\n","   micro avg       0.81      0.78      0.80       134\n","   macro avg       0.40      0.43      0.42       134\n","weighted avg       0.74      0.78      0.76       134\n","\n","\n"]}]},{"cell_type":"code","source":["from collections import defaultdict\n","data_results = defaultdict(list)\n","for target, values in results.items():\n","    for label, scores in values.items():\n","        data_results[\"target\"].append(target)\n","        data_results[\"label\"].append(label)\n","        data_results[\"precision\"].append(scores[\"precision\"])\n","        data_results[\"recall\"].append(scores[\"precision\"])\n","        data_results[\"f1-score\"].append(scores[\"f1-score\"])\n","        data_results[\"support\"].append(scores[\"support\"])\n","        data_results[\"C\"].append(0.1)\n","\n","df_results = pd.DataFrame(data_results)\n","df_results.sort_values(by=['target', 'label'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":833},"id":"j_PlwIufYhHu","executionInfo":{"status":"ok","timestamp":1644677307343,"user_tz":-60,"elapsed":300,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"837cdc13-0480-42ae-b7d3-b37c27f94987"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-b7de8a8f-9db9-47b7-88c6-3d6d03b1bafb\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","      <th>label</th>\n","      <th>precision</th>\n","      <th>recall</th>\n","      <th>f1-score</th>\n","      <th>support</th>\n","      <th>C</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Atheism</td>\n","      <td>AGAINST</td>\n","      <td>0.854167</td>\n","      <td>0.854167</td>\n","      <td>0.809211</td>\n","      <td>160</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Atheism</td>\n","      <td>FAVOR</td>\n","      <td>0.470588</td>\n","      <td>0.470588</td>\n","      <td>0.484848</td>\n","      <td>32</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Atheism</td>\n","      <td>macro avg</td>\n","      <td>0.662377</td>\n","      <td>0.662377</td>\n","      <td>0.647030</td>\n","      <td>192</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Atheism</td>\n","      <td>micro avg</td>\n","      <td>0.780899</td>\n","      <td>0.780899</td>\n","      <td>0.751351</td>\n","      <td>192</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Atheism</td>\n","      <td>weighted avg</td>\n","      <td>0.790237</td>\n","      <td>0.790237</td>\n","      <td>0.755150</td>\n","      <td>192</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>Climate Change is a Real Concern</td>\n","      <td>AGAINST</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>11</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>Climate Change is a Real Concern</td>\n","      <td>FAVOR</td>\n","      <td>0.807692</td>\n","      <td>0.807692</td>\n","      <td>0.830040</td>\n","      <td>123</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>Climate Change is a Real Concern</td>\n","      <td>macro avg</td>\n","      <td>0.403846</td>\n","      <td>0.403846</td>\n","      <td>0.415020</td>\n","      <td>134</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>Climate Change is a Real Concern</td>\n","      <td>micro avg</td>\n","      <td>0.807692</td>\n","      <td>0.807692</td>\n","      <td>0.795455</td>\n","      <td>134</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>Climate Change is a Real Concern</td>\n","      <td>weighted avg</td>\n","      <td>0.741389</td>\n","      <td>0.741389</td>\n","      <td>0.761902</td>\n","      <td>134</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Feminist Movement</td>\n","      <td>AGAINST</td>\n","      <td>0.766234</td>\n","      <td>0.766234</td>\n","      <td>0.700297</td>\n","      <td>183</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Feminist Movement</td>\n","      <td>FAVOR</td>\n","      <td>0.389474</td>\n","      <td>0.389474</td>\n","      <td>0.483660</td>\n","      <td>58</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Feminist Movement</td>\n","      <td>macro avg</td>\n","      <td>0.577854</td>\n","      <td>0.577854</td>\n","      <td>0.591978</td>\n","      <td>241</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>Feminist Movement</td>\n","      <td>micro avg</td>\n","      <td>0.622490</td>\n","      <td>0.622490</td>\n","      <td>0.632653</td>\n","      <td>241</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Feminist Movement</td>\n","      <td>weighted avg</td>\n","      <td>0.675561</td>\n","      <td>0.675561</td>\n","      <td>0.648160</td>\n","      <td>241</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Hillary Clinton</td>\n","      <td>AGAINST</td>\n","      <td>0.712121</td>\n","      <td>0.712121</td>\n","      <td>0.762162</td>\n","      <td>172</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Hillary Clinton</td>\n","      <td>FAVOR</td>\n","      <td>0.535714</td>\n","      <td>0.535714</td>\n","      <td>0.410959</td>\n","      <td>45</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Hillary Clinton</td>\n","      <td>macro avg</td>\n","      <td>0.623918</td>\n","      <td>0.623918</td>\n","      <td>0.586561</td>\n","      <td>217</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Hillary Clinton</td>\n","      <td>micro avg</td>\n","      <td>0.690265</td>\n","      <td>0.690265</td>\n","      <td>0.704289</td>\n","      <td>217</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Hillary Clinton</td>\n","      <td>weighted avg</td>\n","      <td>0.675539</td>\n","      <td>0.675539</td>\n","      <td>0.689332</td>\n","      <td>217</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Legalization of Abortion</td>\n","      <td>AGAINST</td>\n","      <td>0.807692</td>\n","      <td>0.807692</td>\n","      <td>0.730435</td>\n","      <td>189</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Legalization of Abortion</td>\n","      <td>FAVOR</td>\n","      <td>0.490909</td>\n","      <td>0.490909</td>\n","      <td>0.534653</td>\n","      <td>46</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>Legalization of Abortion</td>\n","      <td>macro avg</td>\n","      <td>0.649301</td>\n","      <td>0.649301</td>\n","      <td>0.632544</td>\n","      <td>235</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Legalization of Abortion</td>\n","      <td>micro avg</td>\n","      <td>0.725118</td>\n","      <td>0.725118</td>\n","      <td>0.686099</td>\n","      <td>235</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>Legalization of Abortion</td>\n","      <td>weighted avg</td>\n","      <td>0.745684</td>\n","      <td>0.745684</td>\n","      <td>0.692112</td>\n","      <td>235</td>\n","      <td>0.1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b7de8a8f-9db9-47b7-88c6-3d6d03b1bafb')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b7de8a8f-9db9-47b7-88c6-3d6d03b1bafb button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b7de8a8f-9db9-47b7-88c6-3d6d03b1bafb');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                              target         label  ...  support    C\n","0                            Atheism       AGAINST  ...      160  0.1\n","1                            Atheism         FAVOR  ...       32  0.1\n","3                            Atheism     macro avg  ...      192  0.1\n","2                            Atheism     micro avg  ...      192  0.1\n","4                            Atheism  weighted avg  ...      192  0.1\n","20  Climate Change is a Real Concern       AGAINST  ...       11  0.1\n","21  Climate Change is a Real Concern         FAVOR  ...      123  0.1\n","23  Climate Change is a Real Concern     macro avg  ...      134  0.1\n","22  Climate Change is a Real Concern     micro avg  ...      134  0.1\n","24  Climate Change is a Real Concern  weighted avg  ...      134  0.1\n","15                 Feminist Movement       AGAINST  ...      183  0.1\n","16                 Feminist Movement         FAVOR  ...       58  0.1\n","18                 Feminist Movement     macro avg  ...      241  0.1\n","17                 Feminist Movement     micro avg  ...      241  0.1\n","19                 Feminist Movement  weighted avg  ...      241  0.1\n","5                    Hillary Clinton       AGAINST  ...      172  0.1\n","6                    Hillary Clinton         FAVOR  ...       45  0.1\n","8                    Hillary Clinton     macro avg  ...      217  0.1\n","7                    Hillary Clinton     micro avg  ...      217  0.1\n","9                    Hillary Clinton  weighted avg  ...      217  0.1\n","10          Legalization of Abortion       AGAINST  ...      189  0.1\n","11          Legalization of Abortion         FAVOR  ...       46  0.1\n","13          Legalization of Abortion     macro avg  ...      235  0.1\n","12          Legalization of Abortion     micro avg  ...      235  0.1\n","14          Legalization of Abortion  weighted avg  ...      235  0.1\n","\n","[25 rows x 7 columns]"]},"metadata":{},"execution_count":38}]}]}