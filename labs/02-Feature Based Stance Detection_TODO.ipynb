{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"02-Feature Based Stance Detection_TODO.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"fhxI1uz7gipS"},"source":["# Local Feature Based Stance Detection\n","\n","In this lab session we will implement a very simple linear classifier for Stance Detection using https://scikit-learn.org\n","\n","Stance detection consists of classifying a given document as expressing an AGAINST, FAVOR or NEUTRAL attitude/stance with respect to a given topic. In this particular lab, we use the Task A data from the Semeval 2016 Twitter dataset for Stance detection: https://alt.qcri.org/semeval2016/task6/ \n","\n","Scikit-learn allows you to quickly experiment with a large number of machine learning algorithms in low resource environments (in comparison to neural network approaches). Scikit-learn also provides a large number of functionalities to process data and evaluate and visualize the obtained results.\n","\n","Unlike other toolkits we will see during the course, scikit-learn is a library with an easy to use API ideal for quick experimentation with a large variety of models and algorithms. Usually, it is a good starting point for classification tasks.\n","\n","REMEMBER to check the tutorial:\n","https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html# "]},{"cell_type":"markdown","metadata":{"id":"nrXPDeiBgipd"},"source":["## Load data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eRedGVJFHSdv","executionInfo":{"status":"ok","timestamp":1612344883174,"user_tz":-60,"elapsed":568,"user":{"displayName":"Rodrigo Agerri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW2M3VpBFpNNhe0tx607nu7jJN85RXoeaB4nSw=s64","userId":"18226337086247956839"}},"outputId":"381bc9c1-0636-46da-8ec9-c08ad5d01f72"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"AOEW0KAHgipe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612344884125,"user_tz":-60,"elapsed":1510,"user":{"displayName":"Rodrigo Agerri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW2M3VpBFpNNhe0tx607nu7jJN85RXoeaB4nSw=s64","userId":"18226337086247956839"}},"outputId":"a75bd145-f017-4543-9ede-c6a9050eee37"},"source":["import pandas as pd\n","import nltk\n","import numpy as np\n","from nltk.corpus import stopwords\n","import string\n","from sklearn import preprocessing\n","\n","# download English stopwords\n","nltk.download('stopwords')\n","# download nltk pos tagger for English\n","nltk.download('averaged_perceptron_tagger')\n","\n","# load data\n","def load_data(fnames):\n","    data = []\n","    for fname in fnames:\n","        data.append(pd.read_csv(fname, sep='\\t', encoding='utf-8'))\n","    data = pd.concat(data)\n","    targets = set(data['Target'])\n","    return data, list(targets)\n","\n","def tokenized_tweets(df):\n","    tknzr = nltk.TweetTokenizer()\n","    df['Tokenized_tweet'] = df['Tweet'].apply(tknzr.tokenize)\n","    return df\n","\n","def read_glove(path):\n","    '''\n","    read the glove vectors from path with dimension dim\n","    '''\n","    df = pd.read_csv(path, sep=\" \", quoting=3, header=None, index_col=0)\n","    glove = {key: val.values for key, val in df.T.items()}\n","    return glove\n","\n","def preprocess(data, tokenize=True, remove_stopwords=True, remove_none=True):\n","    if tokenize:\n","        data = tokenized_tweets(data)\n","        data['Clean_tweet'] = data['Tokenized_tweet']\n","    if remove_stopwords:\n","        stop = stopwords.words('english')\n","        data['Clean_tweet'] = data['Clean_tweet'].apply(lambda sentence: [word for word in sentence if word not in stop])\n","        data['Clean_tweet'] = data['Clean_tweet'].apply(lambda sentence: [word for word in sentence if not all([c in string.punctuation for c in word])])\n","    if remove_none:\n","        data = data[data['Stance'] != 'NONE']\n","    return data[['Target','Clean_tweet', 'Stance']]   \n","    \n","def gloveVectorize(glove, text):\n","    '''\n","    Find the pretrained glove vectors of the words in the input text.\n","    The final vector is the average of the vectors\n","    '''\n","    dim = len(glove[\"the\"])\n","    X = np.zeros( (len(text), dim) )\n","    for text_id, t in enumerate(text):\n","        tmp = np.zeros((1, dim))        \n","        # remove oov words\n","        words = [w for w in t if w in glove.keys()]\n","        for word in words:\n","            tmp[:] += glove[word]\n","\n","        if len(words) == 0:\n","            X[text_id, :] = np.zeros((1, dim)) \n","        else:\n","            X[text_id, :] = tmp/len(words)\n","    return X\n","\n","def encode_labels(labels):\n","    enc = preprocessing.LabelEncoder()\n","    encoded = enc.fit_transform(labels)\n","    decoded = enc.inverse_transform(encoded)\n","    return encoded, decoded\n","\n","def data_as_numpy(data):\n","    return np.asarray(data['Tweet']), np.asarray(data['Stance'])\n","\n","def get_table(results):\n","    # print best models results\n","    targts, models, accs = [],[],[]\n","    precs,recs, f1s = [],[], []\n","    par_names, par_values = [], []\n","\n","    for target in results.keys():\n","        for model in results[target]:\n","            targts.append(target)\n","            models.append(model)\n","            accs.append(np.mean(results[target][model]['scores']['test_accuracy']))\n","            precs.append(np.mean(results[target][model]['scores']['test_precision']))\n","            recs.append(np.mean(results[target][model]['scores']['test_recall']))\n","            f1s.append(np.mean(results[target][model]['scores']['test_f1']))\n","            if model == 'rf':\n","                par_names.append('D')\n","                par_values.append(results[target][model]['D'])\n","            else:\n","                par_names.append('C')\n","                par_values.append(results[target][model]['C'])\n","    res_table = pd.DataFrame({'target':targts, 'model':models, \n","                              'accuracy':accs, 'precision':precs, \n","                              'recall':recs, 'fscore':f1s,\n","                              'par_name':par_names, 'par_value':par_values}, columns=['target', 'model', \n","                                                                     'accuracy', 'precision',\n","                                                                     'recall', 'fscore', 'par_name', 'par_value'])\n","    return res_table"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UgBYe_3Jgiph","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1612344884443,"user_tz":-60,"elapsed":1818,"user":{"displayName":"Rodrigo Agerri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW2M3VpBFpNNhe0tx607nu7jJN85RXoeaB4nSw=s64","userId":"18226337086247956839"}},"outputId":"7646fd5b-9cb5-4381-a705-8d54952d71e0"},"source":["# data path. trial data used as training too.\n","trial_file = \"/content/drive/My Drive/Colab Notebooks/2022-ILTAPP/datasets/stance-semeval2016/semeval2016-task6-trialdata.utf-8.txt\"\n","train_file = \"/content/drive/My Drive/Colab Notebooks/2022-ILTAPP/datasets/stance-semeval2016/semeval2016-task6-trainingdata.utf-8.txt\"\n","test_file = \"/content/drive/My Drive/Colab Notebooks/2022-ILTAPP/datasets/stance-semeval2016/SemEval2016-Task6-subtaskA-testdata-gold.txt\"\n","\n","training_data, targets = load_data([trial_file, train_file])\n","training_data = tokenized_tweets(training_data)\n","\n","training_data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Target</th>\n","      <th>Tweet</th>\n","      <th>Stance</th>\n","      <th>Tokenized_tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Hillary Clinton</td>\n","      <td>@tedcruz And, #HandOverTheServer she wiped cle...</td>\n","      <td>AGAINST</td>\n","      <td>[@tedcruz, And, ,, #HandOverTheServer, she, wi...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Hillary Clinton</td>\n","      <td>Hillary is our best choice if we truly want to...</td>\n","      <td>FAVOR</td>\n","      <td>[Hillary, is, our, best, choice, if, we, truly...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Hillary Clinton</td>\n","      <td>@TheView I think our country is ready for a fe...</td>\n","      <td>AGAINST</td>\n","      <td>[@TheView, I, think, our, country, is, ready, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>Hillary Clinton</td>\n","      <td>I just gave an unhealthy amount of my hard-ear...</td>\n","      <td>AGAINST</td>\n","      <td>[I, just, gave, an, unhealthy, amount, of, my,...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>Hillary Clinton</td>\n","      <td>@PortiaABoulger Thank you for adding me to you...</td>\n","      <td>NONE</td>\n","      <td>[@PortiaABoulger, Thank, you, for, adding, me,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   ID  ...                                    Tokenized_tweet\n","0   1  ...  [@tedcruz, And, ,, #HandOverTheServer, she, wi...\n","1   2  ...  [Hillary, is, our, best, choice, if, we, truly...\n","2   3  ...  [@TheView, I, think, our, country, is, ready, ...\n","3   4  ...  [I, just, gave, an, unhealthy, amount, of, my,...\n","4   5  ...  [@PortiaABoulger, Thank, you, for, adding, me,...\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"TLAneW_1gipj"},"source":["## Feature Extraction\n","\n","From [Mohammad et al. 2016](http://saifmohammad.com/WebDocs/1605.01655v1.pdf):\n","\n","The features used in our text classification system are shown below:\n","\n","- __n-grams__: presence or absence of contiguous sequences of 1, 2 and 3 tokens (word n-grams); presence or absence of contiguous sequences of 2, 3, 4, and 5 characters (character n-grams);\n","- __sentiment (sent.)__: The sentiment lexicon features are derived from three manually created lexicons: NRC Emotion Lexicon [Mohammad and Turney 2010], Hu and Liu Lexicon [Hu and Liu 2004], and MPQA Subjectivity Lexicon [Wilson et al. 2005], and two automatically created, tweet-specific, lexicons: NRC Hashtag Sentiment and\n","NRC Emoticon (a.k.a. Sentiment140) [Kiritchenko et al. 2014a]; \n","  + NRC Emotion Lexicon: https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm\n","  + Hu and Liu Lexicon: https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html\n","  + MPQA Subjectivity Lexicon: https://mpqa.cs.pitt.edu/lexicons/subj_lexicon/\n","  + NRC Hashtag Sentiment and NRC Emoticon (a.k.a. Sentiment140): http://saifmohammad.com/WebPages/lexicons.html\n"," \n","\n","- __target__: presence/absence of the target of interest in the tweet;\n","- __POS__: the number of occurrences of each part-of-speech tag (POS);\n","- __encodings (enc.)__: presence/absence of positive and negative emoticons, hashtags, characters in upper case, elongated words (e.g., sweeettt), and punctuations such as exclamation and question marks.\n","\n","----"]},{"cell_type":"markdown","metadata":{"id":"S-sjtzJcgipl"},"source":["# ASSIGNMENT 1\n","\n","We define some helper functions for the feature extraction.\n","\n","+ TODO: define the function to perform POS tagging using NLTK\n","+ TODO: add code to read the NRC-Hashtag-Sentiment-Lexicon-v1.0"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"5KSiM9JUgipn"},"source":["# utils\n","import re\n","import nltk\n","tknzr = nltk.TweetTokenizer() # keep global for simplicity\n","\n","def tokenize(sentence):\n","    return tknzr.tokenize(sentence)\n","\n","def pos_tag(sentence):\n","  # TODO complete this function to perform pos tagging using NLTK\n","\n","# helper functions for feature extraction\n","def ngrams(tokens, n):\n","    return list(zip(*[tokens[i:] for i in range(n)]))\n","\n","def load_nrc_emotions(fname):\n","    emotions = {}\n","    f = open(fname, 'r')\n","    _ = f.readline()\n","    for line in f:\n","        word, emotion, affect = line.rstrip().split('\\t')\n","        if affect == '1':\n","            if word not in emotions:\n","                emotions[word] = []\n","            emotions[word].append(emotion)\n","    f.close()\n","    return emotions\n","\n","def load_nrc_hashtags(fname_list):\n","    sentiment = {}\n","    # TODO add code to read the NRC hashtags file\n","    # HINT: you may inspect the file in the drive\n","    return sentiment\n","\n","def load_nrc_emoticons(fname_list):\n","    emoticons = {}\n","    for fname in fname_list:\n","        f = open(fname, 'r')\n","        for line in f:\n","            emoti, score, _, _ = line.rstrip().split('\\t')\n","            emoticons[emoti] = score\n","        f.close()\n","    return emoticons\n","\n","def load_hu_liu(neg_fname, pos_fname):\n","    sentiments = {}\n","    f = open(neg_fname, 'r')\n","    for line in f:\n","        if re.search('^;', line.rstrip()):\n","            continue\n","        if re.search('^$', line.rstrip()):\n","            continue\n","        sentiments[line.rstrip()] = 'negative'\n","    f.close()\n","    f = open(pos_fname, 'r')\n","    for line in f:\n","        if re.search('^;', line.rstrip()):\n","            continue\n","        if re.search('^$', line.rstrip()):\n","            continue\n","        sentiments[line.rstrip()] = 'positive'\n","    f.close()\n","    return sentiments\n","\n","def load_mpqa_polarities(fname):\n","    polarities = {}\n","    f = open(fname, 'r')\n","    for line in f:\n","        sp  = line.rstrip().split(' ')\n","        if sp[5] == 'm':\n","            word = sp[2].split('=')[1]\n","            polarity = sp[6].split('=')[1]\n","        else:\n","            word = sp[2].split('=')[1]\n","            polarity = sp[5].split('=')[1]\n","        polarities[word] = polarity\n","    f. close()\n","    return polarities\n","\n","\n","def generate_target_dict(targets):\n","    target_dict = {}\n","    stop = stopwords.words('english')\n","    for target in targets:\n","        # original\n","        target_dict[target] = 1\n","\n","        # lower case\n","        target_dict[target.lower()] = 1\n","\n","        # join Hillary Clinton => HillaryClinton\n","        target_dict[target.replace(' ', '')] = 1\n","        target_dict['#'+target.replace(' ', '')] = 1\n","        target_dict['@'+target.replace(' ', '')] = 1\n","        target_dict[target.lower().replace(' ', '')] = 1\n","        target_dict['#'+target.lower().replace(' ', '')] = 1\n","        target_dict['@'+target.lower().replace(' ', '')] = 1\n","\n","        # process parts of target name:\n","        for part in target.split(' '):\n","            if part not in stop:\n","                target_dict[part] = 1\n","                target_dict['#'+part] = 1\n","                target_dict['@'+part] = 1\n","                target_dict['#'+part.lower()] = 1\n","                target_dict['@'+part.lower()] = 1\n","    return target_dict\n","\n","\n","class Match(object):\n","\n","    def __init__(self):\n","        self.trie = {}\n","\n","    def match(self, words):\n","        i = 0\n","        while (i < len(words)):\n","            j = self.match2(words, i, self.trie)\n","            if j >= 0:\n","                return True\n","            i += 1\n","        return False\n","\n","    def match2(self, words, i, trie):\n","        if words[i] not in trie:\n","            return -1\n","        for length in sorted(trie[words[i]].keys(), reverse=True):\n","            context = ' '.join(words[i+1:i+length+1])\n","            for entry in trie[words[i]][length]:\n","                if context == entry:\n","                    return length\n","        return -1\n","\n","    def matchinit(self, dictionary):\n","        for entry in dictionary.keys():\n","            firstword = re.split(' +', entry)[0]\n","            rwords = re.split(' +', entry)[1:]\n","\n","            length = len(rwords)\n","            if firstword not in self.trie:\n","                self.trie[firstword] = {}\n","            if length not in self.trie[firstword]:\n","                self.trie[firstword][length] = []\n","            self.trie[firstword][length].append(' '.join(rwords))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mRftbPMIgipn"},"source":["----\n","\n","We load emotion and sentiment lexicons used in the feature extraction "]},{"cell_type":"code","metadata":{"collapsed":true,"id":"YfCUkYcdgipo"},"source":["# Load lexicon and use as global variables.\n","\n","# NRC emotion lexicon\n","nrc_emotion_file = '/content/drive/My Drive/Colab Notebooks/2022-ILTAPP/datasets/NRC-Emotion-Lexicon-v0.92/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt'\n","nrc_emotions = load_nrc_emotions(nrc_emotion_file)\n","\n","# NRC hashtag sentiment\n","nrc_hashtag_path = '/content/drive/My Drive/Colab Notebooks/2022-ILTAPP/datasets/NRC-Hashtag-Sentiment-Lexicon-v1.0'\n","nrc_ht_sentiments = load_nrc_hashtags([nrc_hashtag_path+'/HS-unigrams.txt', nrc_hashtag_path+'/HS-bigrams.txt' ])\n","\n","# NRC emoticons sentiment\n","nrc_emoticons_path = '/content/drive/My Drive/Colab Notebooks/2022-ILTAPP/datasets/NRC-Emoticon-Lexicon-v1.0'\n","nrc_em_sentiments = load_nrc_emoticons([nrc_emoticons_path+'/Emoticon-unigrams.txt', nrc_emoticons_path+'/Emoticon-bigrams.txt'])\n","\n","# Hu and Liu sentiment lexicon\n","hu_liu_path = '/content/drive/My Drive/Colab Notebooks/2022-ILTAPP/datasets/Hu-Liu_sentiment_lexicon'\n","hu_liu_sentiments = load_hu_liu(hu_liu_path+'/negative-words.utf8.txt', hu_liu_path+'/positive-words.utf8.txt')\n","\n","# MPQA polarity lexicon\n","mpqa_file = '/content/drive/My Drive/Colab Notebooks/2022-ILTAPP/datasets/mpqa_polarities/subjclueslen1-HLTEMNLP05.tff'\n","mpqa_polarities = load_mpqa_polarities(mpqa_file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rzseby-Cgipp"},"source":["----\n","\n","Define feature functions. Function will apply for each instance in the dataset. Output of the functions is a custom python dictionary with the activated/extracted features. Note that after the extraction we'll need to vectorize whole dataset of feature."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"8no7nzaSgipq"},"source":["# features\n","def word_ngrams(tokens, n):\n","    features = {}\n","    name = str(n)+'wgram:'\n","    for ngram in ngrams(tokens, n):\n","        features[name+'_'.join(ngram)] = 1\n","    return features\n","\n","def char_ngrams(sentence, n):\n","    features = {}\n","    name = str(n)+'cgram:'\n","    for ngram in ngrams(sentence, n):\n","        features[name+'_'.join(ngram)] = 1\n","    return features\n","\n","def pos_nb(pos_tags):\n","    features = {}\n","    name='pos_nb:'\n","    for tag in pos_tags:\n","        feat = name+tag[1]\n","        if feat not in features:\n","            features[feat] = 1\n","        else:\n","            features[feat] += 1\n","    return features\n","\n","def target_occurs(sentence, matcher):\n","    features = {}\n","    name = 'target:'\n","    if  matcher.match(sentence):\n","        features[name+'true'] = 1\n","    else:\n","        features[name+'false'] = 1\n","    return features\n","\n","def nrc_emotions_features(tokens, lexicon):\n","    features = {}\n","    name='nrc_emo:'\n","    for token in tokens:\n","        if token in lexicon:\n","            for emotion in lexicon[token]:\n","                features[name+token+':'+emotion] = 1\n","    return features\n","\n","def nrc_hashtag_features(tokens, lexicon):\n","    features = {}\n","    name = 'nrc_ht:'\n","    for token in tokens:\n","        if token in lexicon:\n","            features[name+token] = lexicon[token]\n","    return features\n","\n","def nrc_emoticons_features(tokens, lexicon):\n","    features = {}\n","    name = 'nrc_emc:'\n","    for token in tokens:\n","        if token in lexicon:\n","            features[name+token] = lexicon[token]\n","    return features\n","\n","def hu_liu_sentiment_features(tokens, lexicon):\n","    features = {}\n","    name = 'hu_liu:'\n","    for token in tokens:\n","        if token in lexicon:\n","            features[name+token+':'+lexicon[token]] = 1\n","    return features\n","\n","def mpqa_polarity_features(tokens, lexicon):\n","    features = {}\n","    name = 'mpqa:'\n","    for token in tokens:\n","        if token in lexicon:\n","            features[name+token+':'+lexicon[token]] = 1\n","    return features\n","\n","\n","def encoding(tokens):\n","    pass\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ASSIGNMENT 2\n","\n","+ TODO: extract bigram and trigram character ngram features using the \"char_ngram\" function.\n","+ TODO: extract postag features by using the pos_nb function."],"metadata":{"id":"YP8-aZaBn9Oe"}},{"cell_type":"code","metadata":{"collapsed":true,"id":"jciOyuRjgipt"},"source":["from sklearn.feature_extraction import DictVectorizer\n","\n","def extract_instance_features(instance, target_matcher=None,\n","                              feature_types=None):\n","    if feature_types is None:\n","        feature_types = {'ngrams' : True,\n","                        'cgrams' : True,\n","                        'sentiment' : True,\n","                        'nb_pos' : True,\n","                        'target' : True}\n","    # tokenize\n","    tokenized = tokenize(instance)\n","    \n","    # part-of-speech\n","    pos = nltk.pos_tag(tokenized)\n","    \n","    # extract features\n","    features = {}\n","    \n","    # word n-grams\n","    if feature_types['ngrams']:\n","        features.update(word_ngrams(tokenized, 1))\n","        features.update(word_ngrams(tokenized, 2))\n","        features.update(word_ngrams(tokenized, 3))\n","    \n","    # TODO implement bigram and trigram character n-grams features\n","    \n","    \n","    # sentiment\n","    if feature_types['sentiment']:\n","        features.update(nrc_emotions_features(tokenized, nrc_emotions))\n","        features.update(nrc_hashtag_features(tokenized, nrc_ht_sentiments))\n","        features.update(nrc_emoticons_features(tokenized, nrc_em_sentiments))\n","        features.update(hu_liu_sentiment_features(tokenized, hu_liu_sentiments))\n","        features.update(mpqa_polarity_features(tokenized, mpqa_polarities))\n","    \n","    # TODO extract postag features \n","    \n","    \n","    # target\n","    if feature_types['target'] and target_matcher is not None:\n","        features.update(target_occurs(tokenized, target_matcher))\n","    \n","    return features\n","\n","def extract_features(instances, target_names, \n","                     feature_types=None):\n","    target_dict = generate_target_dict(target_names)\n","    matcher = Match()\n","    matcher.matchinit(target_dict)\n","    features = [extract_instance_features(inst, matcher, feature_types) for inst in instances]\n","    return features"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uXMWb6p8gipu"},"source":["---\n","\n","Example of feature extraction of a single instance."]},{"cell_type":"code","metadata":{"id":"vvpUtXFFgipv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612344886230,"user_tz":-60,"elapsed":3579,"user":{"displayName":"Rodrigo Agerri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW2M3VpBFpNNhe0tx607nu7jJN85RXoeaB4nSw=s64","userId":"18226337086247956839"}},"outputId":"aad00f60-f33f-491e-9581-4c4d343101e1"},"source":["target = 'Hillary Clinton'\n","target_dict = generate_target_dict([target])\n","matcher = Match()\n","matcher.matchinit(target_dict)\n","\n","print(target_dict)\n","print(training_data['Tweet'].iloc[0])\n","feats = extract_instance_features(training_data['Tweet'].iloc[0], matcher)\n","feats"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'Hillary Clinton': 1, 'hillary clinton': 1, 'HillaryClinton': 1, '#HillaryClinton': 1, '@HillaryClinton': 1, 'hillaryclinton': 1, '#hillaryclinton': 1, '@hillaryclinton': 1, 'Hillary': 1, '#Hillary': 1, '@Hillary': 1, '#hillary': 1, '@hillary': 1, 'Clinton': 1, '#Clinton': 1, '@Clinton': 1, '#clinton': 1, '@clinton': 1}\n","@tedcruz And, #HandOverTheServer she wiped clean + 30k deleted emails, explains dereliction of duty/lies re #Benghazi,etc #tcot #SemST\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'1wgram:#Benghazi': 1,\n"," '1wgram:#HandOverTheServer': 1,\n"," '1wgram:#SemST': 1,\n"," '1wgram:#tcot': 1,\n"," '1wgram:+': 1,\n"," '1wgram:,': 1,\n"," '1wgram:/': 1,\n"," '1wgram:30k': 1,\n"," '1wgram:@tedcruz': 1,\n"," '1wgram:And': 1,\n"," '1wgram:clean': 1,\n"," '1wgram:deleted': 1,\n"," '1wgram:dereliction': 1,\n"," '1wgram:duty': 1,\n"," '1wgram:emails': 1,\n"," '1wgram:etc': 1,\n"," '1wgram:explains': 1,\n"," '1wgram:lies': 1,\n"," '1wgram:of': 1,\n"," '1wgram:re': 1,\n"," '1wgram:she': 1,\n"," '1wgram:wiped': 1,\n"," '2cgram: _#': 1,\n"," '2cgram: _+': 1,\n"," '2cgram: _3': 1,\n"," '2cgram: _A': 1,\n"," '2cgram: _c': 1,\n"," '2cgram: _d': 1,\n"," '2cgram: _e': 1,\n"," '2cgram: _o': 1,\n"," '2cgram: _r': 1,\n"," '2cgram: _s': 1,\n"," '2cgram: _w': 1,\n"," '2cgram:#_B': 1,\n"," '2cgram:#_H': 1,\n"," '2cgram:#_S': 1,\n"," '2cgram:#_t': 1,\n"," '2cgram:+_ ': 1,\n"," '2cgram:,_ ': 1,\n"," '2cgram:,_e': 1,\n"," '2cgram:/_l': 1,\n"," '2cgram:0_k': 1,\n"," '2cgram:3_0': 1,\n"," '2cgram:@_t': 1,\n"," '2cgram:A_n': 1,\n"," '2cgram:B_e': 1,\n"," '2cgram:H_a': 1,\n"," '2cgram:O_v': 1,\n"," '2cgram:S_T': 1,\n"," '2cgram:S_e': 1,\n"," '2cgram:T_h': 1,\n"," '2cgram:a_i': 1,\n"," '2cgram:a_n': 1,\n"," '2cgram:a_z': 1,\n"," '2cgram:c_ ': 1,\n"," '2cgram:c_l': 1,\n"," '2cgram:c_o': 1,\n"," '2cgram:c_r': 1,\n"," '2cgram:c_t': 1,\n"," '2cgram:d_ ': 1,\n"," '2cgram:d_,': 1,\n"," '2cgram:d_O': 1,\n"," '2cgram:d_c': 1,\n"," '2cgram:d_e': 1,\n"," '2cgram:d_u': 1,\n"," '2cgram:e_ ': 1,\n"," '2cgram:e_S': 1,\n"," '2cgram:e_a': 1,\n"," '2cgram:e_d': 1,\n"," '2cgram:e_l': 1,\n"," '2cgram:e_m': 1,\n"," '2cgram:e_n': 1,\n"," '2cgram:e_r': 1,\n"," '2cgram:e_s': 1,\n"," '2cgram:e_t': 1,\n"," '2cgram:e_x': 1,\n"," '2cgram:f_ ': 1,\n"," '2cgram:g_h': 1,\n"," '2cgram:h_a': 1,\n"," '2cgram:h_e': 1,\n"," '2cgram:i_,': 1,\n"," '2cgram:i_c': 1,\n"," '2cgram:i_e': 1,\n"," '2cgram:i_l': 1,\n"," '2cgram:i_n': 1,\n"," '2cgram:i_o': 1,\n"," '2cgram:i_p': 1,\n"," '2cgram:k_ ': 1,\n"," '2cgram:l_a': 1,\n"," '2cgram:l_e': 1,\n"," '2cgram:l_i': 1,\n"," '2cgram:l_s': 1,\n"," '2cgram:m_S': 1,\n"," '2cgram:m_a': 1,\n"," '2cgram:n_ ': 1,\n"," '2cgram:n_d': 1,\n"," '2cgram:n_g': 1,\n"," '2cgram:n_s': 1,\n"," '2cgram:o_f': 1,\n"," '2cgram:o_n': 1,\n"," '2cgram:o_t': 1,\n"," '2cgram:p_e': 1,\n"," '2cgram:p_l': 1,\n"," '2cgram:r_ ': 1,\n"," '2cgram:r_T': 1,\n"," '2cgram:r_e': 1,\n"," '2cgram:r_u': 1,\n"," '2cgram:r_v': 1,\n"," '2cgram:s_ ': 1,\n"," '2cgram:s_,': 1,\n"," '2cgram:s_h': 1,\n"," '2cgram:t_ ': 1,\n"," '2cgram:t_c': 1,\n"," '2cgram:t_e': 1,\n"," '2cgram:t_i': 1,\n"," '2cgram:t_y': 1,\n"," '2cgram:u_t': 1,\n"," '2cgram:u_z': 1,\n"," '2cgram:v_e': 1,\n"," '2cgram:w_i': 1,\n"," '2cgram:x_p': 1,\n"," '2cgram:y_/': 1,\n"," '2cgram:z_ ': 1,\n"," '2cgram:z_i': 1,\n"," '2wgram:#Benghazi_,': 1,\n"," '2wgram:#HandOverTheServer_she': 1,\n"," '2wgram:#tcot_#SemST': 1,\n"," '2wgram:+_30k': 1,\n"," '2wgram:,_#HandOverTheServer': 1,\n"," '2wgram:,_etc': 1,\n"," '2wgram:,_explains': 1,\n"," '2wgram:/_lies': 1,\n"," '2wgram:30k_deleted': 1,\n"," '2wgram:@tedcruz_And': 1,\n"," '2wgram:And_,': 1,\n"," '2wgram:clean_+': 1,\n"," '2wgram:deleted_emails': 1,\n"," '2wgram:dereliction_of': 1,\n"," '2wgram:duty_/': 1,\n"," '2wgram:emails_,': 1,\n"," '2wgram:etc_#tcot': 1,\n"," '2wgram:explains_dereliction': 1,\n"," '2wgram:lies_re': 1,\n"," '2wgram:of_duty': 1,\n"," '2wgram:re_#Benghazi': 1,\n"," '2wgram:she_wiped': 1,\n"," '2wgram:wiped_clean': 1,\n"," '3cgram: _#_B': 1,\n"," '3cgram: _#_H': 1,\n"," '3cgram: _#_S': 1,\n"," '3cgram: _#_t': 1,\n"," '3cgram: _+_ ': 1,\n"," '3cgram: _3_0': 1,\n"," '3cgram: _A_n': 1,\n"," '3cgram: _c_l': 1,\n"," '3cgram: _d_e': 1,\n"," '3cgram: _d_u': 1,\n"," '3cgram: _e_m': 1,\n"," '3cgram: _e_x': 1,\n"," '3cgram: _o_f': 1,\n"," '3cgram: _r_e': 1,\n"," '3cgram: _s_h': 1,\n"," '3cgram: _w_i': 1,\n"," '3cgram:#_B_e': 1,\n"," '3cgram:#_H_a': 1,\n"," '3cgram:#_S_e': 1,\n"," '3cgram:#_t_c': 1,\n"," '3cgram:+_ _3': 1,\n"," '3cgram:,_ _#': 1,\n"," '3cgram:,_ _e': 1,\n"," '3cgram:,_e_t': 1,\n"," '3cgram:/_l_i': 1,\n"," '3cgram:0_k_ ': 1,\n"," '3cgram:3_0_k': 1,\n"," '3cgram:@_t_e': 1,\n"," '3cgram:A_n_d': 1,\n"," '3cgram:B_e_n': 1,\n"," '3cgram:H_a_n': 1,\n"," '3cgram:O_v_e': 1,\n"," '3cgram:S_e_m': 1,\n"," '3cgram:S_e_r': 1,\n"," '3cgram:T_h_e': 1,\n"," '3cgram:a_i_l': 1,\n"," '3cgram:a_i_n': 1,\n"," '3cgram:a_n_ ': 1,\n"," '3cgram:a_n_d': 1,\n"," '3cgram:a_z_i': 1,\n"," '3cgram:c_ _#': 1,\n"," '3cgram:c_l_e': 1,\n"," '3cgram:c_o_t': 1,\n"," '3cgram:c_r_u': 1,\n"," '3cgram:c_t_i': 1,\n"," '3cgram:d_ _c': 1,\n"," '3cgram:d_ _e': 1,\n"," '3cgram:d_,_ ': 1,\n"," '3cgram:d_O_v': 1,\n"," '3cgram:d_c_r': 1,\n"," '3cgram:d_e_l': 1,\n"," '3cgram:d_e_r': 1,\n"," '3cgram:d_u_t': 1,\n"," '3cgram:e_ _#': 1,\n"," '3cgram:e_ _w': 1,\n"," '3cgram:e_S_e': 1,\n"," '3cgram:e_a_n': 1,\n"," '3cgram:e_d_ ': 1,\n"," '3cgram:e_d_c': 1,\n"," '3cgram:e_l_e': 1,\n"," '3cgram:e_l_i': 1,\n"," '3cgram:e_m_S': 1,\n"," '3cgram:e_m_a': 1,\n"," '3cgram:e_n_g': 1,\n"," '3cgram:e_r_ ': 1,\n"," '3cgram:e_r_T': 1,\n"," '3cgram:e_r_e': 1,\n"," '3cgram:e_r_v': 1,\n"," '3cgram:e_s_ ': 1,\n"," '3cgram:e_t_c': 1,\n"," '3cgram:e_t_e': 1,\n"," '3cgram:e_x_p': 1,\n"," '3cgram:f_ _d': 1,\n"," '3cgram:g_h_a': 1,\n"," '3cgram:h_a_z': 1,\n"," '3cgram:h_e_ ': 1,\n"," '3cgram:h_e_S': 1,\n"," '3cgram:i_,_e': 1,\n"," '3cgram:i_c_t': 1,\n"," '3cgram:i_e_s': 1,\n"," '3cgram:i_l_s': 1,\n"," '3cgram:i_n_s': 1,\n"," '3cgram:i_o_n': 1,\n"," '3cgram:i_p_e': 1,\n"," '3cgram:k_ _d': 1,\n"," '3cgram:l_a_i': 1,\n"," '3cgram:l_e_a': 1,\n"," '3cgram:l_e_t': 1,\n"," '3cgram:l_i_c': 1,\n"," '3cgram:l_i_e': 1,\n"," '3cgram:l_s_,': 1,\n"," '3cgram:m_S_T': 1,\n"," '3cgram:m_a_i': 1,\n"," '3cgram:n_ _+': 1,\n"," '3cgram:n_ _o': 1,\n"," '3cgram:n_d_,': 1,\n"," '3cgram:n_d_O': 1,\n"," '3cgram:n_g_h': 1,\n"," '3cgram:n_s_ ': 1,\n"," '3cgram:o_f_ ': 1,\n"," '3cgram:o_n_ ': 1,\n"," '3cgram:o_t_ ': 1,\n"," '3cgram:p_e_d': 1,\n"," '3cgram:p_l_a': 1,\n"," '3cgram:r_ _s': 1,\n"," '3cgram:r_T_h': 1,\n"," '3cgram:r_e_ ': 1,\n"," '3cgram:r_e_l': 1,\n"," '3cgram:r_u_z': 1,\n"," '3cgram:r_v_e': 1,\n"," '3cgram:s_ _d': 1,\n"," '3cgram:s_ _r': 1,\n"," '3cgram:s_,_ ': 1,\n"," '3cgram:s_h_e': 1,\n"," '3cgram:t_ _#': 1,\n"," '3cgram:t_c_ ': 1,\n"," '3cgram:t_c_o': 1,\n"," '3cgram:t_e_d': 1,\n"," '3cgram:t_i_o': 1,\n"," '3cgram:t_y_/': 1,\n"," '3cgram:u_t_y': 1,\n"," '3cgram:u_z_ ': 1,\n"," '3cgram:v_e_r': 1,\n"," '3cgram:w_i_p': 1,\n"," '3cgram:x_p_l': 1,\n"," '3cgram:y_/_l': 1,\n"," '3cgram:z_ _A': 1,\n"," '3cgram:z_i_,': 1,\n"," '3wgram:#Benghazi_,_etc': 1,\n"," '3wgram:#HandOverTheServer_she_wiped': 1,\n"," '3wgram:+_30k_deleted': 1,\n"," '3wgram:,_#HandOverTheServer_she': 1,\n"," '3wgram:,_etc_#tcot': 1,\n"," '3wgram:,_explains_dereliction': 1,\n"," '3wgram:/_lies_re': 1,\n"," '3wgram:30k_deleted_emails': 1,\n"," '3wgram:@tedcruz_And_,': 1,\n"," '3wgram:And_,_#HandOverTheServer': 1,\n"," '3wgram:clean_+_30k': 1,\n"," '3wgram:deleted_emails_,': 1,\n"," '3wgram:dereliction_of_duty': 1,\n"," '3wgram:duty_/_lies': 1,\n"," '3wgram:emails_,_explains': 1,\n"," '3wgram:etc_#tcot_#SemST': 1,\n"," '3wgram:explains_dereliction_of': 1,\n"," '3wgram:lies_re_#Benghazi': 1,\n"," '3wgram:of_duty_/': 1,\n"," '3wgram:re_#Benghazi_,': 1,\n"," '3wgram:she_wiped_clean': 1,\n"," '3wgram:wiped_clean_+': 1,\n"," '4cgram: _#_B_e': 1,\n"," '4cgram: _#_H_a': 1,\n"," '4cgram: _#_S_e': 1,\n"," '4cgram: _#_t_c': 1,\n"," '4cgram: _+_ _3': 1,\n"," '4cgram: _3_0_k': 1,\n"," '4cgram: _A_n_d': 1,\n"," '4cgram: _c_l_e': 1,\n"," '4cgram: _d_e_l': 1,\n"," '4cgram: _d_e_r': 1,\n"," '4cgram: _d_u_t': 1,\n"," '4cgram: _e_m_a': 1,\n"," '4cgram: _e_x_p': 1,\n"," '4cgram: _o_f_ ': 1,\n"," '4cgram: _r_e_ ': 1,\n"," '4cgram: _s_h_e': 1,\n"," '4cgram: _w_i_p': 1,\n"," '4cgram:#_B_e_n': 1,\n"," '4cgram:#_H_a_n': 1,\n"," '4cgram:#_S_e_m': 1,\n"," '4cgram:#_t_c_o': 1,\n"," '4cgram:+_ _3_0': 1,\n"," '4cgram:,_ _#_H': 1,\n"," '4cgram:,_ _e_x': 1,\n"," '4cgram:,_e_t_c': 1,\n"," '4cgram:/_l_i_e': 1,\n"," '4cgram:0_k_ _d': 1,\n"," '4cgram:3_0_k_ ': 1,\n"," '4cgram:@_t_e_d': 1,\n"," '4cgram:A_n_d_,': 1,\n"," '4cgram:B_e_n_g': 1,\n"," '4cgram:H_a_n_d': 1,\n"," '4cgram:O_v_e_r': 1,\n"," '4cgram:S_e_m_S': 1,\n"," '4cgram:S_e_r_v': 1,\n"," '4cgram:T_h_e_S': 1,\n"," '4cgram:a_i_l_s': 1,\n"," '4cgram:a_i_n_s': 1,\n"," '4cgram:a_n_ _+': 1,\n"," '4cgram:a_n_d_O': 1,\n"," '4cgram:a_z_i_,': 1,\n"," '4cgram:c_ _#_t': 1,\n"," '4cgram:c_l_e_a': 1,\n"," '4cgram:c_o_t_ ': 1,\n"," '4cgram:c_r_u_z': 1,\n"," '4cgram:c_t_i_o': 1,\n"," '4cgram:d_ _c_l': 1,\n"," '4cgram:d_ _e_m': 1,\n"," '4cgram:d_,_ _#': 1,\n"," '4cgram:d_O_v_e': 1,\n"," '4cgram:d_c_r_u': 1,\n"," '4cgram:d_e_l_e': 1,\n"," '4cgram:d_e_r_e': 1,\n"," '4cgram:d_u_t_y': 1,\n"," '4cgram:e_ _#_B': 1,\n"," '4cgram:e_ _w_i': 1,\n"," '4cgram:e_S_e_r': 1,\n"," '4cgram:e_a_n_ ': 1,\n"," '4cgram:e_d_ _c': 1,\n"," '4cgram:e_d_ _e': 1,\n"," '4cgram:e_d_c_r': 1,\n"," '4cgram:e_l_e_t': 1,\n"," '4cgram:e_l_i_c': 1,\n"," '4cgram:e_m_S_T': 1,\n"," '4cgram:e_m_a_i': 1,\n"," '4cgram:e_n_g_h': 1,\n"," '4cgram:e_r_ _s': 1,\n"," '4cgram:e_r_T_h': 1,\n"," '4cgram:e_r_e_l': 1,\n"," '4cgram:e_r_v_e': 1,\n"," '4cgram:e_s_ _r': 1,\n"," '4cgram:e_t_c_ ': 1,\n"," '4cgram:e_t_e_d': 1,\n"," '4cgram:e_x_p_l': 1,\n"," '4cgram:f_ _d_u': 1,\n"," '4cgram:g_h_a_z': 1,\n"," '4cgram:h_a_z_i': 1,\n"," '4cgram:h_e_ _w': 1,\n"," '4cgram:h_e_S_e': 1,\n"," '4cgram:i_,_e_t': 1,\n"," '4cgram:i_c_t_i': 1,\n"," '4cgram:i_e_s_ ': 1,\n"," '4cgram:i_l_s_,': 1,\n"," '4cgram:i_n_s_ ': 1,\n"," '4cgram:i_o_n_ ': 1,\n"," '4cgram:i_p_e_d': 1,\n"," '4cgram:k_ _d_e': 1,\n"," '4cgram:l_a_i_n': 1,\n"," '4cgram:l_e_a_n': 1,\n"," '4cgram:l_e_t_e': 1,\n"," '4cgram:l_i_c_t': 1,\n"," '4cgram:l_i_e_s': 1,\n"," '4cgram:l_s_,_ ': 1,\n"," '4cgram:m_a_i_l': 1,\n"," '4cgram:n_ _+_ ': 1,\n"," '4cgram:n_ _o_f': 1,\n"," '4cgram:n_d_,_ ': 1,\n"," '4cgram:n_d_O_v': 1,\n"," '4cgram:n_g_h_a': 1,\n"," '4cgram:n_s_ _d': 1,\n"," '4cgram:o_f_ _d': 1,\n"," '4cgram:o_n_ _o': 1,\n"," '4cgram:o_t_ _#': 1,\n"," '4cgram:p_e_d_ ': 1,\n"," '4cgram:p_l_a_i': 1,\n"," '4cgram:r_ _s_h': 1,\n"," '4cgram:r_T_h_e': 1,\n"," '4cgram:r_e_ _#': 1,\n"," '4cgram:r_e_l_i': 1,\n"," '4cgram:r_u_z_ ': 1,\n"," '4cgram:r_v_e_r': 1,\n"," '4cgram:s_ _d_e': 1,\n"," '4cgram:s_ _r_e': 1,\n"," '4cgram:s_,_ _e': 1,\n"," '4cgram:s_h_e_ ': 1,\n"," '4cgram:t_ _#_S': 1,\n"," '4cgram:t_c_ _#': 1,\n"," '4cgram:t_c_o_t': 1,\n"," '4cgram:t_e_d_ ': 1,\n"," '4cgram:t_e_d_c': 1,\n"," '4cgram:t_i_o_n': 1,\n"," '4cgram:t_y_/_l': 1,\n"," '4cgram:u_t_y_/': 1,\n"," '4cgram:u_z_ _A': 1,\n"," '4cgram:v_e_r_ ': 1,\n"," '4cgram:v_e_r_T': 1,\n"," '4cgram:w_i_p_e': 1,\n"," '4cgram:x_p_l_a': 1,\n"," '4cgram:y_/_l_i': 1,\n"," '4cgram:z_ _A_n': 1,\n"," '4cgram:z_i_,_e': 1,\n"," '5cgram: _#_B_e_n': 1,\n"," '5cgram: _#_H_a_n': 1,\n"," '5cgram: _#_S_e_m': 1,\n"," '5cgram: _#_t_c_o': 1,\n"," '5cgram: _+_ _3_0': 1,\n"," '5cgram: _3_0_k_ ': 1,\n"," '5cgram: _A_n_d_,': 1,\n"," '5cgram: _c_l_e_a': 1,\n"," '5cgram: _d_e_l_e': 1,\n"," '5cgram: _d_e_r_e': 1,\n"," '5cgram: _d_u_t_y': 1,\n"," '5cgram: _e_m_a_i': 1,\n"," '5cgram: _e_x_p_l': 1,\n"," '5cgram: _o_f_ _d': 1,\n"," '5cgram: _r_e_ _#': 1,\n"," '5cgram: _s_h_e_ ': 1,\n"," '5cgram: _w_i_p_e': 1,\n"," '5cgram:#_B_e_n_g': 1,\n"," '5cgram:#_H_a_n_d': 1,\n"," '5cgram:#_S_e_m_S': 1,\n"," '5cgram:#_t_c_o_t': 1,\n"," '5cgram:+_ _3_0_k': 1,\n"," '5cgram:,_ _#_H_a': 1,\n"," '5cgram:,_ _e_x_p': 1,\n"," '5cgram:,_e_t_c_ ': 1,\n"," '5cgram:/_l_i_e_s': 1,\n"," '5cgram:0_k_ _d_e': 1,\n"," '5cgram:3_0_k_ _d': 1,\n"," '5cgram:@_t_e_d_c': 1,\n"," '5cgram:A_n_d_,_ ': 1,\n"," '5cgram:B_e_n_g_h': 1,\n"," '5cgram:H_a_n_d_O': 1,\n"," '5cgram:O_v_e_r_T': 1,\n"," '5cgram:S_e_m_S_T': 1,\n"," '5cgram:S_e_r_v_e': 1,\n"," '5cgram:T_h_e_S_e': 1,\n"," '5cgram:a_i_l_s_,': 1,\n"," '5cgram:a_i_n_s_ ': 1,\n"," '5cgram:a_n_ _+_ ': 1,\n"," '5cgram:a_n_d_O_v': 1,\n"," '5cgram:a_z_i_,_e': 1,\n"," '5cgram:c_ _#_t_c': 1,\n"," '5cgram:c_l_e_a_n': 1,\n"," '5cgram:c_o_t_ _#': 1,\n"," '5cgram:c_r_u_z_ ': 1,\n"," '5cgram:c_t_i_o_n': 1,\n"," '5cgram:d_ _c_l_e': 1,\n"," '5cgram:d_ _e_m_a': 1,\n"," '5cgram:d_,_ _#_H': 1,\n"," '5cgram:d_O_v_e_r': 1,\n"," '5cgram:d_c_r_u_z': 1,\n"," '5cgram:d_e_l_e_t': 1,\n"," '5cgram:d_e_r_e_l': 1,\n"," '5cgram:d_u_t_y_/': 1,\n"," '5cgram:e_ _#_B_e': 1,\n"," '5cgram:e_ _w_i_p': 1,\n"," '5cgram:e_S_e_r_v': 1,\n"," '5cgram:e_a_n_ _+': 1,\n"," '5cgram:e_d_ _c_l': 1,\n"," '5cgram:e_d_ _e_m': 1,\n"," '5cgram:e_d_c_r_u': 1,\n"," '5cgram:e_l_e_t_e': 1,\n"," '5cgram:e_l_i_c_t': 1,\n"," '5cgram:e_m_a_i_l': 1,\n"," '5cgram:e_n_g_h_a': 1,\n"," '5cgram:e_r_ _s_h': 1,\n"," '5cgram:e_r_T_h_e': 1,\n"," '5cgram:e_r_e_l_i': 1,\n"," '5cgram:e_r_v_e_r': 1,\n"," '5cgram:e_s_ _r_e': 1,\n"," '5cgram:e_t_c_ _#': 1,\n"," '5cgram:e_t_e_d_ ': 1,\n"," '5cgram:e_x_p_l_a': 1,\n"," '5cgram:f_ _d_u_t': 1,\n"," '5cgram:g_h_a_z_i': 1,\n"," '5cgram:h_a_z_i_,': 1,\n"," '5cgram:h_e_ _w_i': 1,\n"," '5cgram:h_e_S_e_r': 1,\n"," '5cgram:i_,_e_t_c': 1,\n"," '5cgram:i_c_t_i_o': 1,\n"," '5cgram:i_e_s_ _r': 1,\n"," '5cgram:i_l_s_,_ ': 1,\n"," '5cgram:i_n_s_ _d': 1,\n"," '5cgram:i_o_n_ _o': 1,\n"," '5cgram:i_p_e_d_ ': 1,\n"," '5cgram:k_ _d_e_l': 1,\n"," '5cgram:l_a_i_n_s': 1,\n"," '5cgram:l_e_a_n_ ': 1,\n"," '5cgram:l_e_t_e_d': 1,\n"," '5cgram:l_i_c_t_i': 1,\n"," '5cgram:l_i_e_s_ ': 1,\n"," '5cgram:l_s_,_ _e': 1,\n"," '5cgram:m_a_i_l_s': 1,\n"," '5cgram:n_ _+_ _3': 1,\n"," '5cgram:n_ _o_f_ ': 1,\n"," '5cgram:n_d_,_ _#': 1,\n"," '5cgram:n_d_O_v_e': 1,\n"," '5cgram:n_g_h_a_z': 1,\n"," '5cgram:n_s_ _d_e': 1,\n"," '5cgram:o_f_ _d_u': 1,\n"," '5cgram:o_n_ _o_f': 1,\n"," '5cgram:o_t_ _#_S': 1,\n"," '5cgram:p_e_d_ _c': 1,\n"," '5cgram:p_l_a_i_n': 1,\n"," '5cgram:r_ _s_h_e': 1,\n"," '5cgram:r_T_h_e_S': 1,\n"," '5cgram:r_e_ _#_B': 1,\n"," '5cgram:r_e_l_i_c': 1,\n"," '5cgram:r_u_z_ _A': 1,\n"," '5cgram:r_v_e_r_ ': 1,\n"," '5cgram:s_ _d_e_r': 1,\n"," '5cgram:s_ _r_e_ ': 1,\n"," '5cgram:s_,_ _e_x': 1,\n"," '5cgram:s_h_e_ _w': 1,\n"," '5cgram:t_ _#_S_e': 1,\n"," '5cgram:t_c_ _#_t': 1,\n"," '5cgram:t_c_o_t_ ': 1,\n"," '5cgram:t_e_d_ _e': 1,\n"," '5cgram:t_e_d_c_r': 1,\n"," '5cgram:t_i_o_n_ ': 1,\n"," '5cgram:t_y_/_l_i': 1,\n"," '5cgram:u_t_y_/_l': 1,\n"," '5cgram:u_z_ _A_n': 1,\n"," '5cgram:v_e_r_ _s': 1,\n"," '5cgram:v_e_r_T_h': 1,\n"," '5cgram:w_i_p_e_d': 1,\n"," '5cgram:x_p_l_a_i': 1,\n"," '5cgram:y_/_l_i_e': 1,\n"," '5cgram:z_ _A_n_d': 1,\n"," '5cgram:z_i_,_e_t': 1,\n"," 'hu_liu:clean:positive': 1,\n"," 'hu_liu:lies:negative': 1,\n"," 'mpqa:clean:positive': 1,\n"," 'mpqa:duty:neutral': 1,\n"," 'mpqa:lies:negative': 1,\n"," 'nrc_emc:#tcot': '1.481',\n"," 'nrc_emc:+': '0.265',\n"," 'nrc_emc:,': '0.2',\n"," 'nrc_emc:/': '0.396',\n"," 'nrc_emc:30k': '-0.798',\n"," 'nrc_emc:clean': '-0.262',\n"," 'nrc_emc:deleted': '-1.318',\n"," 'nrc_emc:duty': '-0.564',\n"," 'nrc_emc:emails': '-0.049',\n"," 'nrc_emc:etc': '0.317',\n"," 'nrc_emc:explains': '0.58',\n"," 'nrc_emc:lies': '-0.029',\n"," 'nrc_emc:of': '0.031',\n"," 'nrc_emc:re': '0.551',\n"," 'nrc_emc:she': '-0.338',\n"," 'nrc_emc:wiped': '-1.019',\n"," 'nrc_emo:clean:joy': 1,\n"," 'nrc_emo:clean:positive': 1,\n"," 'nrc_emo:clean:trust': 1,\n"," 'nrc_ht:#tcot': '-1.554',\n"," 'nrc_ht:+': '1.275',\n"," 'nrc_ht:,': '0.271',\n"," 'nrc_ht:/': '-0.91',\n"," 'nrc_ht:30k': '0.072',\n"," 'nrc_ht:@tedcruz': '-4.999',\n"," 'nrc_ht:clean': '0.007',\n"," 'nrc_ht:deleted': '-0.672',\n"," 'nrc_ht:duty': '-0.3',\n"," 'nrc_ht:emails': '-0.424',\n"," 'nrc_ht:etc': '-0.582',\n"," 'nrc_ht:explains': '-0.7',\n"," 'nrc_ht:lies': '-1.137',\n"," 'nrc_ht:of': '0.141',\n"," 'nrc_ht:re': '-0.361',\n"," 'nrc_ht:she': '-0.314',\n"," 'nrc_ht:wiped': '-0.383',\n"," 'pos_nb:,': 3,\n"," 'pos_nb:CC': 1,\n"," 'pos_nb:CD': 2,\n"," 'pos_nb:FW': 1,\n"," 'pos_nb:IN': 1,\n"," 'pos_nb:JJ': 1,\n"," 'pos_nb:NN': 6,\n"," 'pos_nb:NNP': 2,\n"," 'pos_nb:NNS': 1,\n"," 'pos_nb:PRP': 1,\n"," 'pos_nb:VB': 1,\n"," 'pos_nb:VBD': 2,\n"," 'pos_nb:VBZ': 2,\n"," 'target:false': 1}"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"0cLRTIEFgipv"},"source":["## Experiments\n","\n","- **Classifier**: Class weigthed Linear SVM. No previous feature scaling is done (previous experiments showed that takes longer learning and performance is lower).\n","- **Experiments**: We'll run experiments in a in-target scenario running independent experiments in each of 5 targets.\n","    - We'll run ablation test on feature types.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oWo8r43xgipw"},"source":["<a id='intarget'></a>\n","### In-target scenario\n","\n","This section is organized in two parts. First, we'll run a linear SVM with all the features and explore the effect of C when using discrete binary features. Second, we'll run an ablation test to measure the impact of each feature type over the whole system.\n","\n","Finally, we'll compare all the results and draw some conclusions.\n","\n","**Results on ablation test**\n","\n","- overall, when we remove nb_pos feature the system improve about 1 point.\n","- character grams are the ones that contribute more in the system. This might because they are the predominant type of features by far.\n","- The rest contributes little or nothing regarding the whole system."]},{"cell_type":"markdown","metadata":{"id":"NE7OHescgipx"},"source":["**Experiments with whole set of features**"]},{"cell_type":"code","metadata":{"id":"db2IYB-cgipx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612344945353,"user_tz":-60,"elapsed":62695,"user":{"displayName":"Rodrigo Agerri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW2M3VpBFpNNhe0tx607nu7jJN85RXoeaB4nSw=s64","userId":"18226337086247956839"}},"outputId":"477f470d-5f31-4b88-e8b5-dc1533ef46ee"},"source":["%%capture --no-stdout\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, make_scorer\n","from sklearn.model_selection import cross_validate\n","from sklearn.feature_extraction import DictVectorizer\n","\n","\n","# Run experiments: preprocess data and glove\n","results = {}\n","for target in targets:\n","    results[target] = {'svm': {'str':'', 'f1':0.0}}\n","\n","scoring = {'accuracy' : 'accuracy',\n","           'f1': 'f1_macro',\n","           'precision':'precision_macro',\n","           'recall' : 'recall_macro'}\n","\n","xval = 5\n","# feature type: ALL\n","feature_types = {'ngrams' : True,\n","                 'cgrams' : True,\n","                 'sentiment' : True,\n","                 'nb_pos' : True,\n","                 'target' : True}\n","\n","for target in targets:\n","    print('Running experiments for \"{}\"'.format(target))\n","    \n","    training_texts, training_labels = data_as_numpy(training_data[training_data['Target'] == target])\n","    training_features = extract_features(training_texts, [target], feature_types)\n"," \n","    #vectorize\n","    vec = DictVectorizer()\n","    train_x = vec.fit_transform(training_features).toarray()\n","    train_y, labels = encode_labels(training_labels)\n","    \n","    print('Training shape: {}'.format(train_x.shape))\n","    \n","    C = [0.1, 0.5, 1, 10]\n","    for c in C:\n","        linear_svm = Pipeline([\n","            #(\"scaler\", StandardScaler()),\n","            (\"lsvc\", LinearSVC(C=c, class_weight='balanced'))\n","        ])\n","        scores = cross_validate(linear_svm, train_x, train_y, cv=xval, scoring=scoring, return_train_score=False)\n","        f1 = np.mean(scores['test_f1'])\n","        print(\"[LinearSVM] C=%f | f1=%f\" %(c,f1))\n","        if f1 > results[target]['svm']['f1']:\n","            results[target]['svm']['f1'] = f1\n","            results[target]['svm']['str'] = \"[LinearSVM] C=%f | f1=%f\" %(c,f1)\n","            results[target]['svm']['C'] = c\n","            results[target]['svm']['scores'] = scores\n","    print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Running experiments for \"Feminist Movement\"\n","Training shape: (664, 95952)\n","[LinearSVM] C=0.100000 | f1=0.541069\n","[LinearSVM] C=0.500000 | f1=0.542015\n","[LinearSVM] C=1.000000 | f1=0.542015\n","[LinearSVM] C=10.000000 | f1=0.542015\n","\n","Running experiments for \"Atheism\"\n","Training shape: (513, 78576)\n","[LinearSVM] C=0.100000 | f1=0.607935\n","[LinearSVM] C=0.500000 | f1=0.607935\n","[LinearSVM] C=1.000000 | f1=0.607935\n","[LinearSVM] C=10.000000 | f1=0.606229\n","\n","Running experiments for \"Climate Change is a Real Concern\"\n","Training shape: (395, 67953)\n","[LinearSVM] C=0.100000 | f1=0.567823\n","[LinearSVM] C=0.500000 | f1=0.567823\n","[LinearSVM] C=1.000000 | f1=0.567823\n","[LinearSVM] C=10.000000 | f1=0.567823\n","\n","Running experiments for \"Legalization of Abortion\"\n","Training shape: (653, 90564)\n","[LinearSVM] C=0.100000 | f1=0.620476\n","[LinearSVM] C=0.500000 | f1=0.621319\n","[LinearSVM] C=1.000000 | f1=0.621319\n","[LinearSVM] C=10.000000 | f1=0.621061\n","\n","Running experiments for \"Hillary Clinton\"\n","Training shape: (689, 97012)\n","[LinearSVM] C=0.100000 | f1=0.610861\n","[LinearSVM] C=0.500000 | f1=0.613418\n","[LinearSVM] C=1.000000 | f1=0.613418\n","[LinearSVM] C=10.000000 | f1=0.613418\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tQp22wTCgipy"},"source":["**Ablation tests**"]},{"cell_type":"code","metadata":{"id":"vv6OvMWigipy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612345186449,"user_tz":-60,"elapsed":303787,"user":{"displayName":"Rodrigo Agerri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW2M3VpBFpNNhe0tx607nu7jJN85RXoeaB4nSw=s64","userId":"18226337086247956839"}},"outputId":"047a3d28-772d-42ec-e259-d88abb949524"},"source":["%%capture --no-stdout\n","# run ablation test over feature types\n","feature_type_names = feature_types.keys()\n","for target in targets:\n","    for feature_type in feature_type_names:\n","        results[target]['svm-'+feature_type] = {'str':'', 'f1':0.0}\n","\n","def init_feature_type():\n","    return {'ngrams' : True,'cgrams' : True,'sentiment' : True,'nb_pos' : True, 'target' : True}\n","\n","scoring = {'accuracy' : 'accuracy',\n","           'f1': 'f1_macro',\n","           'precision':'precision_macro',\n","           'recall' : 'recall_macro'}\n","\n","xval = 5\n","for target in targets:\n","    print('Running experiments for \"{}\"'.format(target))\n","    training_texts, training_labels = data_as_numpy(training_data[training_data['Target'] == target])\n","    \n","    # run ablation in target\n","    for feature_type in feature_type_names:\n","        feature_types = init_feature_type()\n","        # deactivate feature type\n","        model_name = 'svm-'+feature_type\n","        feature_types[feature_type] = False\n","        training_features = extract_features(training_texts, [target], feature_types)\n"," \n","        #vectorize\n","        vec = DictVectorizer()\n","        train_x = vec.fit_transform(training_features).toarray()\n","        train_y, labels = encode_labels(training_labels)\n","        \n","        print('Removed feature type: {}'.format(feature_type))\n","        print('Training shape: {}'.format(train_x.shape))\n","\n","        C = [0.1, 0.5, 1, 10]\n","        for c in C:\n","            linear_svm = Pipeline([\n","                #(\"scaler\", StandardScaler()),\n","                (\"lsvc\", LinearSVC(C=c, class_weight='balanced'))\n","            ])\n","            scores = cross_validate(linear_svm, train_x, train_y, cv=xval, scoring=scoring, return_train_score=False)\n","            f1 = np.mean(scores['test_f1'])\n","            print(\"[LinearSVM] C=%f | f1=%f\" %(c,f1))\n","            if f1 > results[target][model_name]['f1']:\n","                results[target][model_name]['f1'] = f1\n","                results[target][model_name]['str'] = \"[LinearSVM] C=%f | f1=%f\" %(c,f1)\n","                results[target][model_name]['C'] = c\n","                results[target][model_name]['scores'] = scores\n","        print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Running experiments for \"Feminist Movement\"\n","Removed feature type: ngrams\n","Training shape: (664, 70858)\n","[LinearSVM] C=0.100000 | f1=0.537905\n","[LinearSVM] C=0.500000 | f1=0.536513\n","[LinearSVM] C=1.000000 | f1=0.536513\n","[LinearSVM] C=10.000000 | f1=0.535061\n","\n","Removed feature type: cgrams\n","Training shape: (664, 31553)\n","[LinearSVM] C=0.100000 | f1=0.422438\n","[LinearSVM] C=0.500000 | f1=0.421072\n","[LinearSVM] C=1.000000 | f1=0.420049\n","[LinearSVM] C=10.000000 | f1=0.425588\n","\n","Removed feature type: sentiment\n","Training shape: (664, 89538)\n","[LinearSVM] C=0.100000 | f1=0.539214\n","[LinearSVM] C=0.500000 | f1=0.536478\n","[LinearSVM] C=1.000000 | f1=0.536478\n","[LinearSVM] C=10.000000 | f1=0.536478\n","\n","Removed feature type: nb_pos\n","Training shape: (664, 95909)\n","[LinearSVM] C=0.100000 | f1=0.537800\n","[LinearSVM] C=0.500000 | f1=0.537800\n","[LinearSVM] C=1.000000 | f1=0.537800\n","[LinearSVM] C=10.000000 | f1=0.537800\n","\n","Removed feature type: target\n","Training shape: (664, 95950)\n","[LinearSVM] C=0.100000 | f1=0.543405\n","[LinearSVM] C=0.500000 | f1=0.543467\n","[LinearSVM] C=1.000000 | f1=0.543467\n","[LinearSVM] C=10.000000 | f1=0.543467\n","\n","Running experiments for \"Atheism\"\n","Removed feature type: ngrams\n","Training shape: (513, 58286)\n","[LinearSVM] C=0.100000 | f1=0.609097\n","[LinearSVM] C=0.500000 | f1=0.609097\n","[LinearSVM] C=1.000000 | f1=0.610564\n","[LinearSVM] C=10.000000 | f1=0.610564\n","\n","Removed feature type: cgrams\n","Training shape: (513, 25447)\n","[LinearSVM] C=0.100000 | f1=0.548421\n","[LinearSVM] C=0.500000 | f1=0.535866\n","[LinearSVM] C=1.000000 | f1=0.535866\n","[LinearSVM] C=10.000000 | f1=0.535866\n","\n","Removed feature type: sentiment\n","Training shape: (513, 73462)\n","[LinearSVM] C=0.100000 | f1=0.612060\n","[LinearSVM] C=0.500000 | f1=0.612060\n","[LinearSVM] C=1.000000 | f1=0.612060\n","[LinearSVM] C=10.000000 | f1=0.612060\n","\n","Removed feature type: nb_pos\n","Training shape: (513, 78535)\n","[LinearSVM] C=0.100000 | f1=0.609633\n","[LinearSVM] C=0.500000 | f1=0.609633\n","[LinearSVM] C=1.000000 | f1=0.609633\n","[LinearSVM] C=10.000000 | f1=0.605162\n","\n","Removed feature type: target\n","Training shape: (513, 78574)\n","[LinearSVM] C=0.100000 | f1=0.607935\n","[LinearSVM] C=0.500000 | f1=0.607935\n","[LinearSVM] C=1.000000 | f1=0.607935\n","[LinearSVM] C=10.000000 | f1=0.606229\n","\n","Running experiments for \"Climate Change is a Real Concern\"\n","Removed feature type: ngrams\n","Training shape: (395, 52858)\n","[LinearSVM] C=0.100000 | f1=0.565820\n","[LinearSVM] C=0.500000 | f1=0.567548\n","[LinearSVM] C=1.000000 | f1=0.567548\n","[LinearSVM] C=10.000000 | f1=0.567548\n","\n","Removed feature type: cgrams\n","Training shape: (395, 19186)\n","[LinearSVM] C=0.100000 | f1=0.525366\n","[LinearSVM] C=0.500000 | f1=0.530505\n","[LinearSVM] C=1.000000 | f1=0.530505\n","[LinearSVM] C=10.000000 | f1=0.530505\n","\n","Removed feature type: sentiment\n","Training shape: (395, 63906)\n","[LinearSVM] C=0.100000 | f1=0.564100\n","[LinearSVM] C=0.500000 | f1=0.564100\n","[LinearSVM] C=1.000000 | f1=0.564100\n","[LinearSVM] C=10.000000 | f1=0.564100\n","\n","Removed feature type: nb_pos\n","Training shape: (395, 67911)\n","[LinearSVM] C=0.100000 | f1=0.572147\n","[LinearSVM] C=0.500000 | f1=0.573784\n","[LinearSVM] C=1.000000 | f1=0.571886\n","[LinearSVM] C=10.000000 | f1=0.571886\n","\n","Removed feature type: target\n","Training shape: (395, 67951)\n","[LinearSVM] C=0.100000 | f1=0.567823\n","[LinearSVM] C=0.500000 | f1=0.567823\n","[LinearSVM] C=1.000000 | f1=0.567823\n","[LinearSVM] C=10.000000 | f1=0.567823\n","\n","Running experiments for \"Legalization of Abortion\"\n","Removed feature type: ngrams\n","Training shape: (653, 66623)\n","[LinearSVM] C=0.100000 | f1=0.621219\n","[LinearSVM] C=0.500000 | f1=0.616101\n","[LinearSVM] C=1.000000 | f1=0.616101\n","[LinearSVM] C=10.000000 | f1=0.618843\n","\n","Removed feature type: cgrams\n","Training shape: (653, 29231)\n","[LinearSVM] C=0.100000 | f1=0.541251\n","[LinearSVM] C=0.500000 | f1=0.534337\n","[LinearSVM] C=1.000000 | f1=0.535605\n","[LinearSVM] C=10.000000 | f1=0.536692\n","\n","Removed feature type: sentiment\n","Training shape: (653, 85319)\n","[LinearSVM] C=0.100000 | f1=0.620422\n","[LinearSVM] C=0.500000 | f1=0.622008\n","[LinearSVM] C=1.000000 | f1=0.622008\n","[LinearSVM] C=10.000000 | f1=0.622008\n","\n","Removed feature type: nb_pos\n","Training shape: (653, 90521)\n","[LinearSVM] C=0.100000 | f1=0.626313\n","[LinearSVM] C=0.500000 | f1=0.625460\n","[LinearSVM] C=1.000000 | f1=0.625460\n","[LinearSVM] C=10.000000 | f1=0.626632\n","\n","Removed feature type: target\n","Training shape: (653, 90562)\n","[LinearSVM] C=0.100000 | f1=0.626704\n","[LinearSVM] C=0.500000 | f1=0.621061\n","[LinearSVM] C=1.000000 | f1=0.621061\n","[LinearSVM] C=10.000000 | f1=0.621061\n","\n","Running experiments for \"Hillary Clinton\"\n","Removed feature type: ngrams\n","Training shape: (689, 72936)\n","[LinearSVM] C=0.100000 | f1=0.616721\n","[LinearSVM] C=0.500000 | f1=0.613590\n","[LinearSVM] C=1.000000 | f1=0.613590\n","[LinearSVM] C=10.000000 | f1=0.613590\n","\n","Removed feature type: cgrams\n","Training shape: (689, 29596)\n","[LinearSVM] C=0.100000 | f1=0.536649\n","[LinearSVM] C=0.500000 | f1=0.532319\n","[LinearSVM] C=1.000000 | f1=0.523192\n","[LinearSVM] C=10.000000 | f1=0.526454\n","\n","Removed feature type: sentiment\n","Training shape: (689, 91538)\n","[LinearSVM] C=0.100000 | f1=0.615537\n","[LinearSVM] C=0.500000 | f1=0.613830\n","[LinearSVM] C=1.000000 | f1=0.613830\n","[LinearSVM] C=10.000000 | f1=0.613830\n","\n","Removed feature type: nb_pos\n","Training shape: (689, 96968)\n","[LinearSVM] C=0.100000 | f1=0.604291\n","[LinearSVM] C=0.500000 | f1=0.599221\n","[LinearSVM] C=1.000000 | f1=0.601068\n","[LinearSVM] C=10.000000 | f1=0.599885\n","\n","Removed feature type: target\n","Training shape: (689, 97010)\n","[LinearSVM] C=0.100000 | f1=0.611982\n","[LinearSVM] C=0.500000 | f1=0.610861\n","[LinearSVM] C=1.000000 | f1=0.610861\n","[LinearSVM] C=10.000000 | f1=0.610861\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6bjq6Jmin1Xi"},"source":["# ASSIGNMENT 3\n","\n","Create a table to show the ablation results in a table such as the one shown below.\n","\n","+ TODO Create table\n","+ TODO Sort table to show the ablation results ordered by target, fscore and model"]},{"cell_type":"code","metadata":{"id":"TRHpPGe0gip0","colab":{"base_uri":"https://localhost:8080/","height":990},"executionInfo":{"status":"ok","timestamp":1612345186453,"user_tz":-60,"elapsed":303787,"user":{"displayName":"Rodrigo Agerri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhW2M3VpBFpNNhe0tx607nu7jJN85RXoeaB4nSw=s64","userId":"18226337086247956839"}},"outputId":"f17164da-ff4f-4691-8bc9-7f2a73f3402d"},"source":["#TODO sort table to show the ablation results ordered by target, fscore and model."],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","      <th>model</th>\n","      <th>accuracy</th>\n","      <th>precision</th>\n","      <th>recall</th>\n","      <th>fscore</th>\n","      <th>par_name</th>\n","      <th>par_value</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>9</th>\n","      <td>Atheism</td>\n","      <td>svm-sentiment</td>\n","      <td>0.711498</td>\n","      <td>0.723717</td>\n","      <td>0.591839</td>\n","      <td>0.612060</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Atheism</td>\n","      <td>svm-ngrams</td>\n","      <td>0.711498</td>\n","      <td>0.704106</td>\n","      <td>0.591914</td>\n","      <td>0.610564</td>\n","      <td>C</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Atheism</td>\n","      <td>svm-nb_pos</td>\n","      <td>0.709518</td>\n","      <td>0.723226</td>\n","      <td>0.585174</td>\n","      <td>0.609633</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Atheism</td>\n","      <td>svm</td>\n","      <td>0.709556</td>\n","      <td>0.709490</td>\n","      <td>0.587349</td>\n","      <td>0.607935</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Atheism</td>\n","      <td>svm-target</td>\n","      <td>0.709556</td>\n","      <td>0.709490</td>\n","      <td>0.587349</td>\n","      <td>0.607935</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Atheism</td>\n","      <td>svm-cgrams</td>\n","      <td>0.672568</td>\n","      <td>0.635896</td>\n","      <td>0.532931</td>\n","      <td>0.548421</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Climate Change is a Real Concern</td>\n","      <td>svm-nb_pos</td>\n","      <td>0.724051</td>\n","      <td>0.613591</td>\n","      <td>0.561891</td>\n","      <td>0.573784</td>\n","      <td>C</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Climate Change is a Real Concern</td>\n","      <td>svm</td>\n","      <td>0.716456</td>\n","      <td>0.609948</td>\n","      <td>0.555971</td>\n","      <td>0.567823</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>Climate Change is a Real Concern</td>\n","      <td>svm-target</td>\n","      <td>0.716456</td>\n","      <td>0.609948</td>\n","      <td>0.555971</td>\n","      <td>0.567823</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>Climate Change is a Real Concern</td>\n","      <td>svm-ngrams</td>\n","      <td>0.716456</td>\n","      <td>0.609441</td>\n","      <td>0.555420</td>\n","      <td>0.567548</td>\n","      <td>C</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Climate Change is a Real Concern</td>\n","      <td>svm-sentiment</td>\n","      <td>0.711392</td>\n","      <td>0.606231</td>\n","      <td>0.551908</td>\n","      <td>0.564100</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>Climate Change is a Real Concern</td>\n","      <td>svm-cgrams</td>\n","      <td>0.663291</td>\n","      <td>0.573037</td>\n","      <td>0.518366</td>\n","      <td>0.530505</td>\n","      <td>C</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Feminist Movement</td>\n","      <td>svm-target</td>\n","      <td>0.578412</td>\n","      <td>0.562070</td>\n","      <td>0.536912</td>\n","      <td>0.543467</td>\n","      <td>C</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>Feminist Movement</td>\n","      <td>svm</td>\n","      <td>0.576908</td>\n","      <td>0.560766</td>\n","      <td>0.535324</td>\n","      <td>0.542015</td>\n","      <td>C</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Feminist Movement</td>\n","      <td>svm-sentiment</td>\n","      <td>0.573889</td>\n","      <td>0.554682</td>\n","      <td>0.533835</td>\n","      <td>0.539214</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Feminist Movement</td>\n","      <td>svm-ngrams</td>\n","      <td>0.570870</td>\n","      <td>0.555938</td>\n","      <td>0.531830</td>\n","      <td>0.537905</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Feminist Movement</td>\n","      <td>svm-nb_pos</td>\n","      <td>0.572374</td>\n","      <td>0.558517</td>\n","      <td>0.531183</td>\n","      <td>0.537800</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Feminist Movement</td>\n","      <td>svm-cgrams</td>\n","      <td>0.500080</td>\n","      <td>0.449591</td>\n","      <td>0.424500</td>\n","      <td>0.425588</td>\n","      <td>C</td>\n","      <td>10.0</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>Hillary Clinton</td>\n","      <td>svm-ngrams</td>\n","      <td>0.683571</td>\n","      <td>0.675489</td>\n","      <td>0.607889</td>\n","      <td>0.616721</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>Hillary Clinton</td>\n","      <td>svm-sentiment</td>\n","      <td>0.686470</td>\n","      <td>0.678354</td>\n","      <td>0.607977</td>\n","      <td>0.615537</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>Hillary Clinton</td>\n","      <td>svm</td>\n","      <td>0.680683</td>\n","      <td>0.672484</td>\n","      <td>0.604332</td>\n","      <td>0.613418</td>\n","      <td>C</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>Hillary Clinton</td>\n","      <td>svm-target</td>\n","      <td>0.680683</td>\n","      <td>0.671913</td>\n","      <td>0.602398</td>\n","      <td>0.611982</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>Hillary Clinton</td>\n","      <td>svm-nb_pos</td>\n","      <td>0.677753</td>\n","      <td>0.671401</td>\n","      <td>0.595838</td>\n","      <td>0.604291</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>Hillary Clinton</td>\n","      <td>svm-cgrams</td>\n","      <td>0.611055</td>\n","      <td>0.592867</td>\n","      <td>0.520180</td>\n","      <td>0.536649</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>Legalization of Abortion</td>\n","      <td>svm-target</td>\n","      <td>0.672061</td>\n","      <td>0.665726</td>\n","      <td>0.615086</td>\n","      <td>0.626704</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>Legalization of Abortion</td>\n","      <td>svm-nb_pos</td>\n","      <td>0.672096</td>\n","      <td>0.664174</td>\n","      <td>0.615175</td>\n","      <td>0.626632</td>\n","      <td>C</td>\n","      <td>10.0</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>Legalization of Abortion</td>\n","      <td>svm-sentiment</td>\n","      <td>0.669031</td>\n","      <td>0.657666</td>\n","      <td>0.611422</td>\n","      <td>0.622008</td>\n","      <td>C</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Legalization of Abortion</td>\n","      <td>svm</td>\n","      <td>0.667481</td>\n","      <td>0.660273</td>\n","      <td>0.610430</td>\n","      <td>0.621319</td>\n","      <td>C</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Legalization of Abortion</td>\n","      <td>svm-ngrams</td>\n","      <td>0.665966</td>\n","      <td>0.656656</td>\n","      <td>0.611330</td>\n","      <td>0.621219</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>Legalization of Abortion</td>\n","      <td>svm-cgrams</td>\n","      <td>0.598661</td>\n","      <td>0.575281</td>\n","      <td>0.529374</td>\n","      <td>0.541251</td>\n","      <td>C</td>\n","      <td>0.1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                              target          model  ...  par_name  par_value\n","9                            Atheism  svm-sentiment  ...         C        0.1\n","7                            Atheism     svm-ngrams  ...         C        1.0\n","10                           Atheism     svm-nb_pos  ...         C        0.1\n","6                            Atheism            svm  ...         C        0.1\n","11                           Atheism     svm-target  ...         C        0.1\n","8                            Atheism     svm-cgrams  ...         C        0.1\n","16  Climate Change is a Real Concern     svm-nb_pos  ...         C        0.5\n","12  Climate Change is a Real Concern            svm  ...         C        0.1\n","17  Climate Change is a Real Concern     svm-target  ...         C        0.1\n","13  Climate Change is a Real Concern     svm-ngrams  ...         C        0.5\n","15  Climate Change is a Real Concern  svm-sentiment  ...         C        0.1\n","14  Climate Change is a Real Concern     svm-cgrams  ...         C        0.5\n","5                  Feminist Movement     svm-target  ...         C        0.5\n","0                  Feminist Movement            svm  ...         C        0.5\n","3                  Feminist Movement  svm-sentiment  ...         C        0.1\n","1                  Feminist Movement     svm-ngrams  ...         C        0.1\n","4                  Feminist Movement     svm-nb_pos  ...         C        0.1\n","2                  Feminist Movement     svm-cgrams  ...         C       10.0\n","25                   Hillary Clinton     svm-ngrams  ...         C        0.1\n","27                   Hillary Clinton  svm-sentiment  ...         C        0.1\n","24                   Hillary Clinton            svm  ...         C        0.5\n","29                   Hillary Clinton     svm-target  ...         C        0.1\n","28                   Hillary Clinton     svm-nb_pos  ...         C        0.1\n","26                   Hillary Clinton     svm-cgrams  ...         C        0.1\n","23          Legalization of Abortion     svm-target  ...         C        0.1\n","22          Legalization of Abortion     svm-nb_pos  ...         C       10.0\n","21          Legalization of Abortion  svm-sentiment  ...         C        0.5\n","18          Legalization of Abortion            svm  ...         C        0.5\n","19          Legalization of Abortion     svm-ngrams  ...         C        0.1\n","20          Legalization of Abortion     svm-cgrams  ...         C        0.1\n","\n","[30 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"9uNraGvBgiqE"},"source":["# (BONUS) ASSIGNMENT 4: Evaluation on Test Dataset\n","\n","Evaluate the models in Task A: **in-target scenario**. Train model using whole training set and best C value of each target. Evaluate on the test set.\n","  + **HINT**: Look at the CV code to iterate over the targets to train and evaluate.\n","\n","**Use both official scorer and sklearn API for evaluation**.\n","  + Hint: Look at the evaluation in the Text Classifier training with Spacy."]},{"cell_type":"markdown","metadata":{"id":"EOPx-4akgiqE"},"source":["### Task A: in-target scenario\n","\n","- Question: Do we obtain comparable results with respect to Mohammad et al. 2016?\n","- Write a table to summarize the results obtained.\n","\n"]}]}